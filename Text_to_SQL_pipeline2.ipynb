{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jsJxSkiqsaiV",
   "metadata": {
    "id": "jsJxSkiqsaiV"
   },
   "source": [
    "# Dissertation: MSc Health Data Science 2024-2025\n",
    "\n",
    "Author: Laura Khaukha\n",
    "\n",
    "Python version: Python 3.10\n",
    "\n",
    "Description: Please note the subsequent IPython implements the core metholodly for the dissertation project (see section 2.0 in the correspodning text), titled:\n",
    "\n",
    "Schema-Augmented Prompt Engineering for Text-to-SQL Generation over SNOMED CT with T5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d596693-b7d1-4aea-bf15-d5cad35884d1",
   "metadata": {
    "id": "5d596693-b7d1-4aea-bf15-d5cad35884d1"
   },
   "source": [
    "## Section 1.0: Loading/Preprocessing the NHS TRUD DATA into a SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ey81eSt60VXk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing relevant packages, where neccessary\n",
    "# Keep in mind, in this code block it is assumed that the user has not installed any of the requisite packages, included for reproducity reasons.\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pprint\n",
    "!pip install sqlite3\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install ragas datacompy sqlglot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geFk6N170S9j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all neceesssary packages in preperation for the code\n",
    "# Please run this code block for effective and efficient running of the subsequent code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import io, math, warnings, sqlite3\n",
    "from difflib import SequenceMatcher\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import DataCompyScore, LLMSQLEquivalence\n",
    "from datasets import Dataset\n",
    "import sqlglot\n",
    "from sqlglot import expressions as sx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sGrpwxv3ZQ5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As Google Collab is used in this analysis,\n",
    "#this step was used to connect to google drive, where the data was stored locally, in order to import it.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uqsjb5HqAhDb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising/Defining the database/dataset and table names: Descriptions relationships language refset\n",
    "#This was done due to prior challenges in\n",
    "DB_PATH=\"snomedct_data.db\"\n",
    "DESC_TBL=\"TRUD_descriptiontable\"\n",
    "REL_TBL=\"TRUD_relationshiptable\"\n",
    "LANG_TBL=\"TRUD_language_refset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ysc5VTCb4zSO",
   "metadata": {
    "id": "ysc5VTCb4zSO"
   },
   "source": [
    "1.1 Section: Loading/Preprocessing the SNOMED-CT data from NHS TRUD into the SQLlite dataset\n",
    "\n",
    "- The preprocessing requirements are outliened and reasoned in the corresponding text in section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7e555-c98c-48e9-ad10-8f1169cd6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and Loading the description table\n",
    "decrip_df = pd.read_csv(\"Terminology/sct2_Description_UKCLSnapshot-en_GB1000000_20241120.txt\", sep=\"\\t\", dtype=str)\n",
    "decrip_df.columns = map(str.lower, decrip_df.columns) #Standardise column names, making everything lowercase for consistency\n",
    "decrip_df = decrip_df[decrip_df[\"active\"] == \"1\"] #Filtering the active entries to include for the model, take away the inactive ones\n",
    "\n",
    "#Processing and loading the relationships table\n",
    "rela_df = pd.read_csv(\"SnomedCT_UKClinicalRF2_PRODUCTION_20241120T000001Z/Snapshot/Terminology/sct2_Relationship_UKCLSnapshot_GB1000000_20241120.txt\", sep=\"\\t\", dtype=str)\n",
    "rela_df.columns = map(str.lower, rela_df.columns) #Standardise column names, making everything lowercase for consistency\n",
    "rela_df = rela_df[rela_df[\"active\"] == \"1\"]  #Filtering the active entries to include for the model, take away the inactive ones\n",
    "\n",
    "#Preprocessing and loading the language tables\n",
    "langu_df = pd.read_csv(\"SnomedCT_UKClinicalRF2_PRODUCTION_20241120T000001Z/Snapshot/Refset/Language/der2_cRefset_LanguageUKCLSnapshot-en_GB1000000_20241120.txt\",sep=\"\\t\", dtype=str)\n",
    "langu_df.columns = langu_df.columns.str.lower()\n",
    "langu_df = langu_df[langu_df[\"active\"] == \"1\"]\n",
    "\n",
    "# Creating and connecting to the SQLite database; Loading the Snomed-CT dataset, NHS from TRUD, into a SQLite database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Rewriting the imported dataframes to SQLite tables\n",
    "decrip_df.to_sql(DESC_TBL, conn, if_exists=\"replace\", index=False)\n",
    "rela_df.to_sql(REL_TBL,  conn, if_exists=\"replace\", index=False)\n",
    "langu_df.to_sql(LANG_TBL, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "#commiting the changes made to the DB_Path database, defined upstream of the code, and closing the connection\n",
    "conn.commit()\n",
    "#Viewing whether the changes have been succesfully committed\n",
    "print(\"SQLite tables:\",DB_PATH,\"tables:\",DESC_TBL,REL_TBL,LANG_TBL)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560cc88-3194-4b01-91ef-35aef8c43ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting SQLite database contents,\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#Looking at the insital 10 rows to verify\n",
    "sqllite_df = pd.read_sql_query(\"SELECT * FROM TRUD_descriptiontable LIMIT 10\", conn)\n",
    "print(sqllite_df.head())\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbb7ea-d5b6-48cb-8f9f-5823594852e5",
   "metadata": {
    "id": "f5dbb7ea-d5b6-48cb-8f9f-5823594852e5"
   },
   "source": [
    "## Section 2.0 EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc09bd-ea35-4e8b-b747-923c44c8b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaining initial insights on the dataset: Viewing the dataset\n",
    "conn_1 = sqlite3.connect(DB_PATH)\n",
    "pd.read_sql_query(\"SELECT * FROM TRUD_descriptiontable LIMIT 5\", conn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc79d5-9233-4d47-b2cd-c192a763c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarily,finding terms most commonnly used in the dataseet, for prompt generation in section 3.0 of the script\n",
    "sql = \"\"\"SELECT term, COUNT(*) as frequency FROM TRUD_descriptiontable GROUP BY term ORDER BY frequency DESC LIMIT 20\"\"\"\n",
    "temrs_most = pd.read_sql_query(sql, conn)\n",
    "temrs_most"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wpfLcjZGvaMm",
   "metadata": {
    "id": "wpfLcjZGvaMm"
   },
   "source": [
    "# Section 3: Manually creating the NL-SQL example categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hEttQWQ7vdoR",
   "metadata": {
    "id": "hEttQWQ7vdoR"
   },
   "source": [
    " - Manually creating the NL-SQL example categories (Corresponding to the Method section 3.4, Table 1 in the Text), in preperation for    \n",
    "   the evalaution of shema-augmented prompting on the SNOMED CT data,\n",
    "  via both few-shot (See Section:...) and zero-shot learning with the specific varaint of the T5-small model (See Section: ....).\n",
    "  \n",
    "- Overview of the tasks each gold standard example category aims to  \n",
    "  fullfill (Aiming to reflect common Database to SNOMED CT funciotning):\n",
    "\n",
    "* Category 1: Concept Retrieval of human-readable Terms\n",
    "* Category 2: Synonym and Description Lookup\n",
    "* Category 3: TypeID and ConceptID Extraction from Term\n",
    "* Category 4: Hierarchical Relationship Exploration\n",
    "* Category 5: Aggregation and Calculation-based Queries\n",
    "* Category 6: Module ID-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b17d04-86fb-42b0-8c6e-3aa2e10dc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "slq_promts_category1 = [\n",
    "    (\"What is the preferred term for concept ID 466891000000108?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '466891000000108' AND typeid = '900000000000003001'\"),\n",
    "    (\"What is the full name for concept ID 386721000000105?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '386721000000105' AND typeid = '900000000000003001'\"),\n",
    "    (\"Find the preferred name of concept ID 413681000000103.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '413681000000103' AND typeid = '900000000000013009'\"),\n",
    "    (\"What is the English name of concept ID 466871000000109 ?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '466871000000109' AND languagecode = 'en' AND typeid = '900000000000003001'\"),\n",
    "    (\"Give the preferred description of concept ID 470791000000108.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '470791000000108' AND typeid = '900000000000003001'\"),\n",
    "    (\"What description is used as the preferred term for concept ID 368851000000106?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '368851000000106' AND typeid = '900000000000003001'\"),\n",
    "    (\"What is the label of concept 401991000000109?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '401991000000109' AND typeid = '900000000000003001'\"),\n",
    "    (\"Get the preferred English label for conceptID 431821000000108.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '431821000000108' AND typeid = '900000000000003001' AND languagecode = 'en'\"),\n",
    "    (\"What name is preferred for the concept 443791000000100?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '443791000000100' AND typeid = '900000000000013009'\"),\n",
    "    (\"Give me the preferred description for concept 467081000000103.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '467081000000103' AND typeid = '900000000000003001'\"),\n",
    "    (\"Which term is listed as preferred for concept 401881000000100?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '401881000000100' AND typeid = '900000000000003001'\"),\n",
    "    (\"For concept ID 465351000000104 what is the preferred term ?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '465351000000104' AND typeid = '900000000000003001'\"),\n",
    "    (\"Show me which term corresponds to conceptid 271411000000102\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '271411000000102' AND typeid = '900000000000013009'\"),\n",
    "    (\"Provide the english term of concept ID 441901000000108 ?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '441901000000108' AND languagecode = 'en' AND typeid = '900000000000003001'\"),\n",
    "    (\"Provide the preferred description for the concept ID 19551000000101.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '19551000000101' AND typeid = '900000000000003001'\"),\n",
    "    (\"Which descriptions are used for concept ID 368951000000101?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '368951000000101' AND typeid = '900000000000003001'\"),\n",
    "    (\"Retrieve the label of concept 401761000000100?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE concecptid = '401761000000100' AND typeid = '900000000000003001'\"),\n",
    "    (\"I want the english term for concept ID 467211000000103\t.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '467211000000103' AND typeid = '900000000000003001' AND languagecode = 'en'\"),\n",
    "    (\"Which preferred name corresponds to concept 20121000000101?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '20121000000101' AND typeid = '900000000000013009'\"),\n",
    "    (\"Find the description for concept 253571000000106.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '253571000000106' AND typeid = '900000000000003001'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37829166-4ae0-4e7a-8a11-d62ec771ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_promts_category2 = [\n",
    "    (\"What are all known synonyms for concept ID 427301000000100.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '427301000000100'\"),\n",
    "    (\"List all the different descriptions for concept 270721000000106?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '270721000000106'\"),\n",
    "    (\"Get all English terms for the concept ID 452161000000101.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '452161000000101' AND languagecode = 'en'\"),\n",
    "    (\"Retrieve all labels for concept ID 411771000000105.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '411771000000105'\"),\n",
    "    (\"Show all descriptions including synonyms for concept 441591000000105.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '441591000000105'\"),\n",
    "    (\"What are the known terms for concept ID 470961000000100?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '470961000000100'\"),\n",
    "    (\"Give me all descriptions linked to concept 491431000000104.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '491431000000104\"),\n",
    "    (\"Get every English label for concept ID 401971000000105.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '401971000000105' AND languagecode = 'en'\"),\n",
    "    (\"Return all terms used to describe concept 486081000000102.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '486081000000102'\"),\n",
    "    (\"List every description for concept 418751000000106.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '418751000000106'\"),\n",
    "    (\"Find every synonym for concept ID 487501000000104.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '487501000000104'\"),\n",
    "    (\"Show all the descriptions for concept 271321000000100?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '271321000000100'\"),\n",
    "    (\"Get all English terms for the concept ID 271751000000109.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '271751000000109' AND languagecode = 'en'\"),\n",
    "    (\"I want all labels for concept ID 455621000000108.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '455621000000108'\"),\n",
    "    (\"Display all the descriptions including synonyms for concept 368961000000103\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '368961000000103'\"),\n",
    "    (\"Find all the terms associated with conceptID 455821000000109?\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '455821000000109'\"),\n",
    "    (\"Fetch all the terms linked to concept 467251000000104.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '467251000000104\"),\n",
    "    (\"Provide each english label for concept ID 255871000000109.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '255871000000109' AND languagecode = 'en'\"),\n",
    "    (\"Return all terms used to describe concept 443991000000102.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '443991000000102'\"),\n",
    "    (\"List every description for concept 467591000000109.\",\n",
    "     \"SELECT term FROM TRUD_descriptiontable WHERE conceptid = '467591000000109'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e34f6-e59b-4d00-bbcf-826785ce33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_promts_category3 = [\n",
    "    (\"What is the typeid associated with 'Hypodermic needle injury'?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Hypodermic needle injury'\"),\n",
    "    (\"Is 'Accident caused by sharp-edged object' a synonym or a preferred term?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Accident caused by sharp-edged object'\"),\n",
    "    (\"Retrieve the typeid for the term 'High cost drugs'.\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'High cost drugs'\"),\n",
    "    (\"What is the type for the description 'Maltreatment'?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Maltreatment'\"),\n",
    "    (\"Provide the description type of 'High cost chemotherapy drugs'.\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'High cost chemotherapy drugs'\"),\n",
    "    (\"What is the typeid for the term 'Transarterial approach'?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Transarterial approach'\"),\n",
    "    (\"Which type ID corresponds to the term 'Reconstruction of cranial defect'?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Reconstruction of cranial defect'\"),\n",
    "    (\"What typeid is 'Upper limb' ?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Upper limb'\"),\n",
    "    (\"Find the typeid for 'Excision of lesion of brain tissue'.\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Excision of lesion of brain tissue' \"),\n",
    "    (\"What is the description type for 'Distraction osteogenesis of bones of skull'?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Distraction osteogenesis of bones of skull'\"),\n",
    "    (\"Find the conceptid for 'Operations on bones of skull'?\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Operations on bones of skull'\"),\n",
    "    (\"Is 'Operations for disorders of sex development' a synonym or a preferred term?\",\n",
    "     \"SELECT typeid FROM TRUD_descriptiontable WHERE term = 'Operations for disorders of sex development'\"),\n",
    "    (\"List the conceptid's associated with the term 'Destruction of fetus'.\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Destruction of fetus'\"),\n",
    "    (\"What is the conceptid for 'Wound microscopy, culture and sensitivities'?\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Wound microscopy, culture and sensitivities'\"),\n",
    "    (\"Display the conceptid for'Urine maltose level'.\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Urine maltose level'\"),\n",
    "    (\"Which concpetid corresponds with the term 'Urinary microscopy, culture and sensitivities'?\",\n",
    "     \"SELECT concpetid FROM TRUD_descriptiontable WHERE term = 'Urinary microscopy, culture and sensitivities'\"),\n",
    "    (\"Which conceptid is asssocaited with the term 'Treponema pallidum particle agglutination test'?\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Treponema pallidum particle agglutination test'\"),\n",
    "    (\"Under what conceptid is 'Transplantation of stomach' ?\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Transplantation of stomach'\"),\n",
    "    (\"Return the conceptid for 'Therapeutic endoscopic operations on nasal cavity'.\",\n",
    "     \"SELECT conceptid FROM TRUD_descriptiontable WHERE term = 'Therapeutic endoscopic operations on nasal cavity'\"),\n",
    "    (\"Which conceptid is 'Sickle solubility test'?\",\n",
    "     \"SELECT conceptid  FROM TRUD_descriptiontable WHERE term = 'Sickle solubility test'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ed05f-db29-40da-b178-82c92295521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompts_category4 = [\n",
    "    (\"Which concepts are associated with concept ID 466891000000108?\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '466891000000108'\"),\n",
    "    (\"Provide the destination for  386721000000105.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '386721000000105'\"),\n",
    "    (\"What are the source concepts pointing to 256691000000101?\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '256691000000101'\"),\n",
    "    (\"Provide all the sources concept 413681000000103 connects to.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '413681000000103'\"),\n",
    "    (\"Which concepts are the targets of 466871000000109?\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '466871000000109'\"),\n",
    "    (\"Show all concepts that link to destination 4707910000001081.\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '470791000000108'\"),\n",
    "    (\"Identify the source concepts associated with destination ID 368851000000106.\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '368851000000106'\"),\n",
    "    (\"Which concepts are connected to 401991000000109 as destination?\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '401991000000109'\"),\n",
    "    (\"Show all concepts related from source 468611000000102.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '468611000000102'\"),\n",
    "    (\"List the concepts that 431821000000108 refers to.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '431821000000108'\"),\n",
    "    (\"Which concepts reference 443791000000100 as a destination?\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '443791000000100'\"),\n",
    "    (\"List all destination concepts for concept 467081000000103.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '467081000000103'\"),\n",
    "    (\"Get all related concepts from 401881000000100.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '401881000000100'\"),\n",
    "    (\"Find all links that point to destination 465351000000104.\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '465351000000104'\"),\n",
    "    (\"Return all source IDs for destination concept 271411000000102.\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '271411000000102'\"),\n",
    "    (\"Find the destination ids that link to 441901000000108\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '441901000000108'\"),\n",
    "    (\"Find all concepts that map to 19551000000101.\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '19551000000101'\"),\n",
    "    (\"What are the source concepts that lead to 368951000000101?\",\n",
    "     \"SELECT sourceid FROM TRUD_relationshiptable WHERE destinationid = '368951000000101'\"),\n",
    "    (\"Give me all destination concepts connected from 401761000000100.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '401761000000100'\"),\n",
    "    (\"List concepts related from 467211000000103.\",\n",
    "     \"SELECT destinationid FROM TRUD_relationshiptable WHERE sourceid = '467211000000103'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec12f70-614f-46fc-ac75-f6ec3a4c2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompts_category5= [\n",
    "    (\"How many distinct relationships exist in the table?\",\n",
    "     \"SELECT COUNT(*) FROM TRUD_relationshiptable\"),\n",
    "    (\"How many destination concepts are linked from concept ID 466891000000108?\",\n",
    "     \"SELECT COUNT(destinationid) FROM TRUD_relationshiptable WHERE sourceid = '466891000000108'\"),\n",
    "    (\"Count how many sourceids are assocaited with concept ID 368951000000101.\",\n",
    "     \"SELECT COUNT(sourceid) FROM TRUD_relationshiptable WHERE destinationid = '368951000000101'\"),\n",
    "    (\"Provide the total number of the unique sourceids from the relationship table.\",\n",
    "     \"SELECT COUNT(DISTINCT sourceid) FROM TRUD_relationshiptable\"),\n",
    "    (\"What is the maximum number of conceptids in the table?\",\n",
    "     \"SELECT MAX(conceptid) FROM TRUD_descriptiontable\"),\n",
    "    (\"What is the mimimum number of conceptid in the table?\",\n",
    "     \"SELECT MIN(conceptid) FROM TRUD_descriptiontable\"),\n",
    "    (\"How many destinationid's are present in the schema ?\",\n",
    "     \"SELECT COUNT(destinationid) FROM TRUD_descriptiontable\"),\n",
    "    (\"Find the sum of terms associated with the concpet id 466891000000108\",\n",
    "     \"SELECT SUM(conceptid) FROM TRUD_relationshiptable WHERE conceptid = 466891000000108\"),\n",
    "    (\"Provide total count of conceptids that start with the number 4\",\n",
    "     \"SELECT COUNT(conceptid) FROM TRUD_relationshiptable WHERE conceptid LIKE '4%'\"),\n",
    "    (\"Retrive total number of terms that start with the number heart\",\n",
    "     \"SELECT COUNT(term) FROM TRUD_relationshiptable WHERE term LIKE = 'heart%'\"),\n",
    "    (\"How many terms with the conceptid 466891000000108, end with injury?\",\n",
    "      \"SELECT terms FROM TRUD_relationshiptable WHERE term LIKE = '%injury' AND conceptid = 466891000000108\"),  #11\n",
    "    (\"Find the maximum number of relationships per source ID.\",\n",
    "     \"SELECT MAX(relationship_count) FROM (SELECT sourceid, COUNT(*) AS relationship_count FROM TRUD_relationshiptable GROUP BY sourceid) AS counts\"),\n",
    "    (\"Count the number of relationships per type ID.\",\n",
    "     \"SELECT typeid, COUNT(*) FROM TRUD_relationshiptable GROUP BY typeid\"),\n",
    "    (\"Count how many descriptions contain the word 'disease'\",\n",
    "     \"SELECT COUNT(*) FROM TRUD_descriptiontable WHERE term LIKE '%disease%'\"),\n",
    "    (\"What is the number of relationships per type ID ?.\",\n",
    "     \"SELECT typeid, COUNT(*) FROM TRUD_relationshiptable GROUP BY typeid\"),\n",
    "    (\"Find the source ID with the fewest destination concepts.\",\n",
    "     \"SELECT sourceid, COUNT(destinationid) AS count_dest FROM TRUD_relationshiptable GROUP BY sourceid ORDER BY count_dest ASC LIMIT 1\"),\n",
    "    (\"Get the total number of descriptions per concept ID.\",\n",
    "     \"SELECT conceptid, COUNT(*) AS description_count FROM TRUD_descriptiontable GROUP BY conceptid\"),\n",
    "    (\"How many terms are associated with each language code?\",\n",
    "     \"SELECT languagecode, COUNT(*) FROM TRUD_descriptiontable GROUP BY languagecode\"),\n",
    "    (\"Which language code has the highest number of terms?\",\n",
    "     \"SELECT languagecode, COUNT(*) AS term_count FROM TRUD_descriptiontable GROUP BY languagecode ORDER BY term_count DESC LIMIT 1\"),\n",
    "    (\"What is the maximum concept ID value in the description table?\",\n",
    "     \"SELECT MAX(CAST(conceptid AS UNSIGNED)) FROM TRUD_descriptiontable\"),\n",
    "    (\"What is the minimum concept ID value in the description table?\",\n",
    "     \"SELECT MIN(CAST(conceptid AS UNSIGNED)) FROM TRUD_descriptiontable\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63861ed9-3de9-48c2-a4d4-39552f343239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prompts_category6 = [(\"What are all terms with module ID 999000041000000102?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000041000000102'\"),\n",
    "                         (\"What are all terms with module ID 900000000000207008?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '900000000000207008'\"),\n",
    "                         (\"What are all terms with module ID 999000031000000106?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000031000000106'\"),\n",
    "                         (\"What are all terms with module ID 999000021000000104?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000021000000104'\"),\n",
    "                         (\"What are all terms with module ID 999000011000000101?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000011000000101'\"),\n",
    "                         (\"What are all terms with module ID 999000001000000107?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000001000000107'\"),\n",
    "                         (\"What are all terms with module ID 999000051000000100?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000051000000100'\"),\n",
    "                         (\"What are all terms with module ID 999000061000000105?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000061000000105'\"),\n",
    "                         (\"What are all terms with module ID 999000071000000108?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000071000000108'\"),\n",
    "                         (\"What are all terms with module ID 999000081000000103?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000081000000103'\"),\n",
    "                         (\"What are all terms with module ID 999000091000000106?\",\n",
    "                           \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000091000000106'\"),\n",
    "                        (\"What are all terms with module ID 999000101000000109?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000101000000109'\"),\n",
    "                         (\"What are all terms with module ID 999000111000000105?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000111000000105'\"),\n",
    "                         (\"What are all terms with module ID 999000121000000108?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000121000000108'\"),\n",
    "                         (\"What are all terms with module ID 999000131000000101?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000131000000101'\"),\n",
    "                         (\"What are all terms with module ID 999000141000000103?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000141000000103'\"),\n",
    "                         (\"What are all terms with module ID 999000211000000107?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000211000000107'\"),\n",
    "                         (\"What are all terms with module ID 999000221000000109?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000221000000109'\"),\n",
    "                         (\"What are all terms with module ID 999000231000000104?\",\n",
    "                          \"SELECT term FROM TRUD_descriptiontable WHERE moduleid = '999000231000000104'\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBXvPM9RjZ3f",
   "metadata": {
    "id": "bBXvPM9RjZ3f"
   },
   "source": [
    "# Section 4 Descritpitve statisitcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O6I10J0azZG7",
   "metadata": {
    "id": "O6I10J0azZG7"
   },
   "source": [
    "- The findings of the  descriptive anlysis, are outlined in section 3.2\n",
    "  in the results section page 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nGYcjVZs9Ntt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapping the SQL example categories, NL-SQL paris, to humanreadable cateory names, needed for the descriptive statstics section\n",
    "categories = {\"Concept lookups (Cat: 1)\":slq_promts_category1,\n",
    "              \"Synonyms & descriptions (Cat:2)\":sql_promts_category2,\n",
    "              \"typeId & conceptID by term (Cat3)\":sql_promts_category3,\n",
    "              \"Concept relationships (Cat4)\":sql_prompts_category4,\n",
    "              \"aggregation &counts (Cat5)\":sql_prompts_category5,\n",
    "              \"ModuleId filters (Cat6)\":sql_prompts_category6,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbWnlk83iyks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definining the function validor, to ensure all the cateogories are in the required formate\n",
    "def _validat_(x):\n",
    "    \"\"\"The following function, validator, checks whether an object is valid,\n",
    "     example (NL-SQL) tuple (this is the later required in the prompting pipeliens\n",
    "     \"\"\"\n",
    "    return (\n",
    "        isinstance(x, tuple)\n",
    "        and len(x) ==2\n",
    "        and all(isinstance(t, str) and len(t.strip()) > 0 for t in x))\n",
    "\n",
    "#Defining empty lists for to append descriptive analysis into\n",
    "r_=[]\n",
    "inval_r = []\n",
    "\n",
    "#Looping through each NL-SQL category, computing decriptive statistics\n",
    "for cateogryname, i in categories.items():\n",
    "    nrofitems=len(i)\n",
    "    nrofval =sum(_validat_(x) for x in i)\n",
    "    n_inval= nrofitems - nrofval\n",
    "    r_.append({\n",
    "        \"Category\": cateogryname,\n",
    "        \"Number(Nr.)of items\": nrofitems,\n",
    "        \"Number (Nr.)of valid (NL-SQL pairs)\":nrofval,\n",
    "        \"Number(Nr.) of invalid (NL-SQL pairs)\": n_inval,\n",
    "        \"Percentage of valid NL-SQL examples\":(nrofval / nrofitems) * 100 if nrofitems else 0.0,\n",
    "    })\n",
    "\n",
    "#Displaying the results to report in the corresponding dissertation text, in table 2 in section 3.2 on page 13\n",
    "ds_f = pd.DataFrame(r_).sort_values(\"Category\").reset_index(drop=True)\n",
    "ds_f[\"Percentage of valid NL-SQL examples\"] = ds_f[\"Percentage of valid NL-SQL examples\"].round(1)\n",
    "print(\"Table 2. Distribution of NL–SQL pairs by category\")\n",
    "display(ds_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KBsStvr7kE5h",
   "metadata": {
    "id": "KBsStvr7kE5h"
   },
   "source": [
    "## Section 5: Importing the T5-small model: Evaluation functions\n",
    "\n",
    "\n",
    "1.   Pretrained model Details can be found:\n",
    "     https://huggingface.co/cssupport/t5-small-awesome-text-to-sql\n",
    "\n",
    "     -  The reasoning behind choosing the T5-small may be found in the\n",
    "        corresponding dissertation: See section 2.1 and figure 2 and its figure legend.\n",
    "\n",
    "2.   The evaluation and helper functions for the both the few-shot and\n",
    "     zero-shot pipelines are also defined in the following section.Though notbaly, within the few-shot pipeline especially helper function had to be specialised for the particular cateogory. These are accordinlgy, these are defined and used in the corresponding pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8qbhq6KeHxxC",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the model into the environment\n",
    "tokeniser = T5Tokenizer.from_pretrained('t5-small') #tokeniser\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53vfkb1nlxv_",
   "metadata": {
    "id": "53vfkb1nlxv_"
   },
   "source": [
    "Section 5.1: Defining helper functions,formatting reslults, converting NL into SQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "la9ggNHAjzqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that takes NL input and ouputs SQL , for both zero and few shot pipeline\n",
    "def gensql(inpt_pp):\n",
    "  \"\"\" The following function leverages a specified model, and converts the inputs NL\n",
    "    into a corresponding SQL query\n",
    "  \"\"\"\n",
    "  inpt = tokeniser(inpt_pp, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  with torch.no_grad():\n",
    "    outputs = model.generate(**inpt, max_length=512)\n",
    "    return tokeniser.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#Formatting function for the obtained resutls\n",
    "def runthesql_ctdf(sql: str, conn: sqlite3.Connection) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    The following gunction, executes an obtained SQL query agaisnt the database\n",
    "    (Neccessary for the execuitabllity metrics), thereby returning the result as a dataframe.\n",
    "    If execution has failed None is returend, while empty results return header only dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ot = pd.read_sql_query(sql, conn)\n",
    "        ot = ot.astype(str)\n",
    "        ot = ot.reindex(sorted(ot.columns), axis=1)\n",
    "        if ot.shape[0] ==0:\n",
    "            return ot.head(0)\n",
    "        ot = ot.sort_values(list(ot.columns)).reset_index(drop=True)\n",
    "        return ot\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "#Fromatting the subsequent results dataframeinto CSV text\n",
    "def dtcsvtxt(dt: pd.DataFrame | None) -> str:\n",
    "    \"\"\"\n",
    "    The following function/step is neccessary to compute the DataCompy scores, as it\n",
    "    converts the results dataframe into a CSVfile with header. Due to previous\n",
    "    challenges encountered in computing the Datacompy scores, the function is designed so that\n",
    "    an empty is never returned, and instead provides a messsage, indiating the that the obtained\n",
    "    SQL query given is invalid/non runable.\n",
    "    \"\"\"\n",
    "    if dt is None:\n",
    "        return \"not runnable sql\"\n",
    "    if dt.shape[0] ==0:\n",
    "        return dt.head(0).to_csv(index=False)\n",
    "    return dt.to_csv(index=False)\n",
    "\n",
    "#Function used later to semantic equivalance, ensures standardised fromat\n",
    "def sqlglotr(sql: str):\n",
    "    \"\"\"\n",
    "    The follwoing function is critcial and aid the subsequent\n",
    "    measuring semantic equivalance using sqlglot, through\n",
    "    pareses a SELECT clause into a canonical representation.\n",
    "    It captures which tables are involved, ignoring case and order.\n",
    "    It uses a bollean to flag for the DISTINCT clause, morevoer it\n",
    "    normalises SQL expressions selected by the query, with any aliases removed.\n",
    "    Next normalises list of columns or expressions grouped by, ignoring order.\n",
    "    Lastly, this function normamalsises the WHERE claue.\n",
    "    Overall, this aids in measuring semantic equivalence by ignoring irrelvant differces\n",
    "    in formatting or aliasing, caputuring the queries true intent wrt the reference set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tr = sqlglot.parse_one(sql, read=\"sqlite\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    #allowing answer for SELECT clause\n",
    "    sl = tr.find(sx.Select)\n",
    "    if sl is None:\n",
    "        return None\n",
    "    # Removing Order by, thereby ignoring roworder differences\n",
    "    for ob in list(tr.find_all(sx.Order)):\n",
    "        ob.parent.set(\"order\", None)\n",
    "    #lowercasing tables\n",
    "    table = {t.name.lower() for t in tr.find_all(sx.Table)}\n",
    "    #Flagging the dstinct clause\n",
    "    isdistinct = bool(sl.args.get(\"distinct\", False))\n",
    "    # Removing aliases while normalising the SQL queries to strings,lastly putting these into set\n",
    "    pjn = set()\n",
    "    for e in sl.expressions:\n",
    "        if isinstance(e, sx.Alias):\n",
    "            e = e.this\n",
    "        pjn.add(e.sql(dialect=\"sqlite\", normalize=True).lower())\n",
    "    # group by, normalising strings\n",
    "    gb = sl.args.get(\"group\")\n",
    "    gp_n= set()\n",
    "    if gb:\n",
    "        for g in gb.expressions:\n",
    "            gp_n.add(g.sql(dialect=\"sqlite\", normalize=True).lower())\n",
    "    #normasing the WHERE CLAUSE\n",
    "    whexpr = sl.args.get(\"where\")\n",
    "    whn = whexpr.sql(dialect=\"sqlite\", normalize=True).lower() if whexpr else \"\"\n",
    "    # signature is a tuple of hashables\n",
    "    return (frozenset(table),isdistinct, frozenset(pjn), frozenset(gp_n), whn)\n",
    "\n",
    "\n",
    "def extractsql(text: str) -> str:\n",
    "  \"\"\" the following function extract the generated SQL query usinng Regex logic\"\"\"\n",
    "  t = text.strip()\n",
    "  if \";\" in t: t = t.split(\";\")[0]\n",
    "  m = re.search(r\"(?is)\\bselect\\b.*\", t)\n",
    "  return m.group(0).strip() if m else t\n",
    "\n",
    "def valexecut(sql: str, conn: sqlite3.Connection):\n",
    "    s = sql.strip().rstrip(\";\")\n",
    "    if not re.match(r\"(?is)^\\s*(select|with)\\b\", s):\n",
    "        return False, \"Does not start with SELECT/WITH\"\n",
    "    try:\n",
    "        conn.execute(f\"EXPLAIN QUERY PLAN {s}\")\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False, f\"EXPLAIN failed: {e}\"\n",
    "#Defining paramaters\n",
    "nr_bm=6\n",
    "max_tok=128\n",
    "np_ngram = 3\n",
    "\n",
    "def generatesqldecoderul(p):\n",
    "    \"\"\"Functions specfying the strict decoding rules for the model, while runing the model\"\"\"\n",
    "    inputs = tokeniser(p, padding=False, truncation=False, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=nr_bm,\n",
    "            max_new_tokens=max_tok,\n",
    "            no_repeat_ngram_size=np_ngram,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokeniser.pad_token_id,\n",
    "            bad_words_ids=Bd_w,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tokeniser.decode(out_ids[0], skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vQHVb1KslwHd",
   "metadata": {
    "id": "vQHVb1KslwHd"
   },
   "source": [
    "5.2 Defining Evaluation metric functions: Used throughout both few & zero shot pipelies (Section 6 and 7), respectivly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJNQ6pY-hEGN",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to coput semantic equivalence using the aformentioned sqlglotr\n",
    "\n",
    "def sqlglotsemanticequiv(asql: str, bsql: str) -> float:\n",
    "    \"\"\"\n",
    "    The following function computes the semantic equivalence between the expected\n",
    "    versus the obtained SQL queries. Its a binary metric, if the the semantic\n",
    "    equivalence is has macthces, it returnes 1 otherwise the parsing failes, thereby\n",
    "    returning 0, null\n",
    "    \"\"\"\n",
    "    asig = sqlglotr(asql)\n",
    "    bsig = sqlglotr(bsql)\n",
    "    return 1.0 if (asig is not None and asig==bsig) else 0.0\n",
    "\n",
    "#Adds the computeded semantic equivalence into a column in the results dataframse\n",
    "def add_sqlglot_equiv(dtin: pd.DataFrame, execo: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The follwoing function adds the computeded semantic equivalence into a column,\n",
    "    if execo is True, non-executable rows get NaN, thereby excluding them from means\n",
    "    Otherwise every SQL results will be scored, if they fail to parse failures, null\n",
    "    \"\"\"\n",
    "    dt = dtin.copy()\n",
    "    vl = []\n",
    "    for _, r in dt.iterrows():\n",
    "        if execo and not r.get(\"exec_ok\", False):\n",
    "            vl.append(np.nan)\n",
    "        else:\n",
    "            vl.append(sqlglotsemanticequiv(r[\"generated_sql\"], r[\"expected_sql\"]))\n",
    "    dt[\"sqlglot_equiv\"] = vl\n",
    "    return dt\n",
    "\n",
    "def add_datacompy_scores(df_results: pd.DataFrame,conn: sqlite3.Connection,mode: str = \"rows\", metric: str = \"f1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The following function adds a datacompy score column to the obtained results dataframse,\n",
    "    comparing executed result sets. Hereby hanlding empty/non-executable cases\n",
    "    to avoid NaNs and ZeroDivisionError, which had to implemented due to early work experiments.\n",
    "    \"\"\"\n",
    "    # Normalize to ragas-expected\n",
    "    mode = {\"row\":\"rows\",\"rows\":\"rows\",\"column\":\"columns\",\"columns\":\"columns\"}[mode]\n",
    "    metric = {\"f1\":\"f1\",\"precision\":\"precision\",\"recall\":\"recall\"}[metric]\n",
    "    sr = DataCompyScore(mode=mode, metric=metric)\n",
    "    out = df_results.copy()\n",
    "    sr: list[float] = []\n",
    "    for _, r in out.iterrows():\n",
    "        exp_df = runthesql_ctdf(r[\"expected_sql\"], conn)\n",
    "        gen_df = runthesql_ctdf(r[\"generated_sql\"], conn)\n",
    "        #if SQL is non execuitbale return 0\n",
    "        if exp_df is None or gen_df is None:\n",
    "            sr.append(0.0)\n",
    "            continue\n",
    "        #Degenerating the empty cases\n",
    "        n_exp, n_gen = exp_df.shape[0], gen_df.shape[0]\n",
    "        if n_exp == 0 and n_gen == 0:\n",
    "            sr.append(1.0)  # both empty → perfect match\n",
    "            continue\n",
    "        if n_gen == 0 and n_exp > 0:\n",
    "            sr.append(1.0 if metric == \"precision\" else 0.0)\n",
    "            continue\n",
    "        if n_exp == 0 and n_gen > 0:\n",
    "            sr.append(1.0 if metric == \"recall\" else 0.0)\n",
    "            continue\n",
    "        # Normal case\n",
    "        sample = SingleTurnSample(\n",
    "            response= dtcsvtxt(gen_df),\n",
    "            reference= dtcsvtxt(exp_df)\n",
    "        )\n",
    "        try:\n",
    "            s = sr.single_turn_score(sample)\n",
    "            s = float(s)\n",
    "            if math.isnan(s) or math.isinf(s):\n",
    "                s = 0.0\n",
    "        except ZeroDivisionError:\n",
    "            s = 0.0\n",
    "        except Exception:\n",
    "            s = 0.0\n",
    "        sr.append(s)\n",
    "    out[\"datacompy_score\"] = sr\n",
    "    return out\n",
    "\n",
    " #Function which taggs those queries that are able to run against the database in the subsequent piplines\n",
    "def tagexpairs(df_in: pd.DataFrame, conn) -> pd.DataFrame:\n",
    "    \"\"\" The following function marks those SQL pairs that are able to\n",
    "        run against the database \"\"\"\n",
    "    rows = []\n",
    "    for _, r in df_in.iterrows():\n",
    "        exp_df = runthesql_ctdf(r[\"expected_sql\"], conn)\n",
    "        gen_df = runthesql_ctdf(r[\"generated_sql\"], conn)\n",
    "        rows.append(((exp_df is not None), (gen_df is not None)))\n",
    "    df_out = df_in.copy()\n",
    "    exp_ok, gen_ok = zip(*rows)\n",
    "    df_out[\"exec_ok\"] = [eo and go for eo, go in zip(exp_ok, gen_ok)]\n",
    "    return df_out\n",
    "\n",
    "#Sequencence match metric (ASMS in the report)\n",
    "def sequencematch(a: str, b: str) -> float:\n",
    "  \"\"\" The following function computes the sequence match metric values in the susbequent pipelines: ASMS in the report\"\"\"\n",
    "  return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# Function to computes the exact match metric (EMA in the report)\n",
    "def EMA(expsql: str, gensql: str, conn: sqlite3.Connection) -> bool:\n",
    "    \"\"\"The following function computes the exact match metric values in the subsequent pipelines\"\"\"\n",
    "    expdt = runthesql_ctdf(expsql, conn)\n",
    "    gendt = runthesql_ctdf(gensql, conn)\n",
    "    return (expdt is not None) and (gendt is not None) and expdt.equals(gendt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TZLTZnjBZK1b",
   "metadata": {
    "id": "TZLTZnjBZK1b"
   },
   "source": [
    "# Section 6.0: FEW-shot Prompting & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KtZSWe873JCx",
   "metadata": {
    "id": "KtZSWe873JCx"
   },
   "source": [
    "Please note that this an extraction helper function for few-shot prompting was experimented with, which did improve the overall quality of the SQL results, across all metrics. However as it was considered not to reflect the models true peformance, instead inflated results values due to what was considered minor data leakage, it was removed.\n",
    "\n",
    "Extractor function Looked like, however were adapted depening on the taks requirements for category 3 for instance the function was adapted to extract the term in the NL question:\n",
    "\n",
    "def extractconceptidfNL(text: str):\n",
    "    m = re.search(r\"\\b(\\d{8,20})\\b\", text)\n",
    "    return m.group(1) if m else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XV8melL8qhY",
   "metadata": {
    "id": "6XV8melL8qhY"
   },
   "source": [
    "Section 6.1 Few-shot prompting for Category 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8UB8K6fH6nzq",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not already open the database\n",
    "conn_eval = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Predefining th\n",
    "FSN = \"900000000000003001\" #Predefining the Fully Specified Name\n",
    "SYN = \"900000000000013009\" #Predefinong the Synonym/preferred term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354wKsX3bqk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function below parses the NL questionto determine  extra filters for typeid and laguage codes, based on the aformentioned key words\n",
    "def cat1filters(nl: str):\n",
    "    \"\"\" The follwoing function parses the NL question, in oder to determine\n",
    "    extra filters for typeid and laguage codes, based on the aformentioned\n",
    "    key words. Thereby returning a list of column,value pairs.\n",
    "    \"\"\"\n",
    "    t = nl.lower()\n",
    "    wh= []\n",
    "    if \"full name\" in t or \"fsn\" in t:\n",
    "      wh.append((\"typeid\", FSN))\n",
    "    elif \"preferred\" in t or \"synonym\" in t:\n",
    "      wh.append((\"typeid\", SYN))\n",
    "    else:# default to FSN if nothing else is specifiec\n",
    "      wh.append((\"typeid\", FSN))\n",
    "    if \"english\" in t:\n",
    "        wh.append((\"languagecode\", \"en\"))\n",
    "    return wh\n",
    "\n",
    "# Establishing a set of rules the model must follow, created based on previous model struggles in SQL generation\n",
    "def rulescat1(nl: str, concept_id: str | None) -> list[str]:\n",
    "    \"\"\" The following function enforces rules the model must follow for category 1 SQL generation.\n",
    "       This was created to guide the model in SQL generation, since the same errors were continously observed\n",
    "       in SQl ouptuts from the model\n",
    "    \"\"\"\n",
    "    muss = [\n",
    "        \"Only use table: TRUD_descriptiontable\",\n",
    "        \"Only use columns: conceptid, term, typeid, languagecode\",\n",
    "    ]\n",
    "    # Encforcing rules on conceptid: A common struggle observed when SQL was created\n",
    "    if concept_id:\n",
    "        muss.append(\"Concept IDs are TEXT: wrap them in single quotes\")\n",
    "        muss.append(f\"Must use conceptid = '{concept_id}' exactly (do not change digits)\")\n",
    "    # NL-derived filters (FSN/SYN + optional English)\n",
    "    muss += [f\"Must include {col} = '{val}'\" for col, val in cat1filters(nl)]\n",
    "    # Retruning the textual description\n",
    "    muss.append(\"Return exactly one SELECT statement (no comments)\")\n",
    "    muss.append(\"Do not use JOINs or other tables\")\n",
    "    muss.append(\"Must select 'term'\")\n",
    "    return muss\n",
    "\n",
    "# The following function verifies if any of the aforementioned rules were violated (rulescat1)\n",
    "def violates_cat1(sql: str, nl: str, concept_id: str) -> str:\n",
    "    \"\"\"The following function verifies if any of the aforementioned rules were\n",
    "    violated during SQL generation\"\"\"\n",
    "    s = sql.strip()\n",
    "    lo = s.lower()\n",
    "    #Select clause must be used\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s):\n",
    "        return \"Not a SELECT\"\n",
    "    #The correct table must be selected as follows\n",
    "    if \"trud_relationshiptable\" in lo:\n",
    "        return \"Used TRUD_relationshiptable\"\n",
    "    if \"trud_descriptiontable\" not in lo:\n",
    "        return \"Missing TRUD_descriptiontable\"\n",
    "    #Preventing commonly detected hallucinated columns\n",
    "    badcols = {\n",
    "        \"destinationid\",\"sourceid\",\"relationid\",\"productid\",\"prescriptionid\",\n",
    "        \"termid\",\"conjectid\",\"concertid\",\"label\",\"description\",\"column\",\"columns\",\"chart\",\"table\",\"name\"\n",
    "    }\n",
    "    for c in badcols:\n",
    "        if re.search(rf\"\\b{re.escape(c)}\\b\", lo):\n",
    "            return f\"Used non-existent/forbidden column '{c}'\"\n",
    "    # Requiring the exact match of the concept id\n",
    "    if concept_id and not re.search(rf\"conceptid\\s*=\\s*'{re.escape(concept_id)}'\", s, flags=re.I):\n",
    "        return \"Missing or mutated conceptid predicate\"\n",
    "    # Requiring the mapped predicates from cat1filters\n",
    "    for col, val in cat1filters(nl):\n",
    "        if not re.search(rf\"\\b{re.escape(col)}\\s*=\\s*'{re.escape(val)}'\", s, flags=re.I):\n",
    "            return f\"Missing required predicate {col} = '{val}'\"\n",
    "    #Term must also be selected\n",
    "    if not re.search(r\"(?is)^\\s*select\\s+(distinct\\s+)?term\\b\", s):\n",
    "        return \"Must select 'term'\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "#specifying the schema hint\n",
    "s_hint = (\n",
    "    \"You write SQLite SQL.\\n\"\n",
    "    \"Return exactly one SELECT statement. No comments.\\n\"\n",
    "    \"Use only the tables and columns shown. Do not invent tables or columns.\")\n",
    "\n",
    "#The following function formats the examples into the valid few-shot prompt format\"\n",
    "def buildfewpromptcat(eg, cuestion, schematxt, concept_id, k=3):\n",
    "    \"\"\"The following function formats the examples into the valid few-shot prompt format\"\"\"\n",
    "    rul = \"\\n- \".join(rules5(cuestion, concept_id))\n",
    "    dem = \"\\n\\n\".join([f\"query for: {q}\\nSQL: {sql.strip()}\" for q, sql in eg[:k]])\n",
    "    q = cuestion.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{s_hint}\\n\\nRules:\\n- {rul}\\n\\n\"\n",
    "        f\"tables:\\n{schematxt}\\n\\n\"\n",
    "        f\"Here are some examples:\\n{dem}\\n\\n\"\n",
    "        f\"Now query for: {q}?\\nSQL:\"\n",
    "    )\n",
    "\n",
    "#Function below serves as a preventative measure for hallucation, by forbidding the model to generate certain coulumns names\n",
    "def badwords(tokeniser):\n",
    "    \"\"\" The following function aids the model in preveting hallucatoin, by forbidding the model to generate certain coulumns\n",
    "           names: based on observations in previous attempts.\n",
    "    \"\"\"\n",
    "    ban =[\"column\",\"columns\",\"chart\",\"table\",\"destinationid\",\"sourceid\",\"relationid\",\n",
    "          \"productid\",\"prescriptionid\",\"termid\",\"conjectid\",\"concertid\",\"label\",\"description\",\"name\"]\n",
    "    ids =[]\n",
    "    for w in ban:\n",
    "        t =tokeniser(w, add_special_tokens=False).input_ids\n",
    "        if len(t) ==1: ids.append(t)\n",
    "    return ids\n",
    "#applying this function\n",
    "Bd_w= badwords(tokeniser)\n",
    "\n",
    "# The following function generates an SQL query from a natural language prompt, retrying twice\n",
    "def generatewithretry(prompt, nl, concept_id, maxtries=2):\n",
    "    \"\"\" The following function generates an SQL query from a natural language prompt using a language model,\n",
    "        and retries generation up to `maxtries` times if the output is still invalid\n",
    "    \"\"\"\n",
    "    lastsql, lastreason = \"\", \"\"\n",
    "    p = prompt\n",
    "    for _ in range(maxtries):\n",
    "        sql = extractsql(generatesqldecoderul(p))\n",
    "        reas = violates_cat1(sql, nl, concept_id)\n",
    "        good, whynot = valexecut(sql, conn_eval)\n",
    "        if not reas and good:\n",
    "            return sql, \"good\"\n",
    "        lastsql, lastreason = sql, (reas or whynot)\n",
    "        p += f\"\\n\\n previous output invalid: {lastreason}. Obey all Rules strictly.\"\n",
    "    return lastsql, f\"failed despite retries: {lastreason}\"\n",
    "\n",
    "schem = (\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT);\")\n",
    "\n",
    "\n",
    "schem = (\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT);\")\n",
    "\n",
    "#empty list for the resuttls\n",
    "rfew =[]\n",
    "#Looping through category 1 via few-shot prompting: generating results\n",
    "for i, (q, expected_sql) in enumerate(slq_promts_category1):\n",
    "    concept_id = None\n",
    "    #Specfiying the number of examples the model is show (K), excluding the current item\n",
    "    examples = [p for j,p in enumerate(slq_promts_category1) if j != i][:3]\n",
    "    promptfs = buildfewpromptcat(examples, q, schem, concept_id, k=3)\n",
    "    gensql, status = generatewithretry(promptfs, q, concept_id, maxtries=2)\n",
    "    rfew.append({\n",
    "        \"Description\": q,\n",
    "        \"Prompt\": promptfs,\n",
    "        \"Generated sql\": gensql,\n",
    "        \"Expected sql\": expected_sql.strip(),\n",
    "        \"Status\": status})\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt1 = pd.DataFrame(rfew)\n",
    "dt1 = dt1.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt1[\"exact match\"] = dt1.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt1[\"Sequence match (ASMS)\"] = dt1.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt1[\"EMA\"] = dt1.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact Match Accuracy: {dt1['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt1['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt1['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores,\n",
    "dt1_dc =add_datacompy_scores(dt1, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt1dcprec = add_datacompy_scores(dt1, conn_eval, mode=\"rows\",metric=\"precision\")\n",
    "dt1dcrec  = add_datacompy_scores(dt1, conn_eval, mode=\"rows\",metric=\"recall\")\n",
    "dt1dccol  = add_datacompy_scores(dt1, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt1_exc = tagexpairs(dt1_dc, conn_eval)\n",
    "dt1_exc= add_sqlglot_equiv(dt1_exc,execo=True)\n",
    "mask = dt1_exc[\"sqlglot_equiv\"].notna()\n",
    "\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {dt1_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(dt1_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", dt1_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\",dt1dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", dt1dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", dt1dccol[\"datacompy_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epiY5U8FQvBJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the category 1 resutls into a csv file\n",
    "dt1.to_csv(\"categoy1results.csv\", index=False)\n",
    "#Downloading the results for cateogry 1 into the local device: This specifically for google Collab\n",
    "files.download(\"categoy1results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53OhQtIkDZ5m",
   "metadata": {
    "id": "53OhQtIkDZ5m"
   },
   "source": [
    "Section 6.2 Few-shot prompting for Category 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NLzp9pzTuFnO",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying addtional filters for Category 2 queries based on NL\n",
    "def cat2filters(nl: str):\n",
    "    \"\"\"\n",
    "    The follwoing function parses the NL question, in oder to determine\n",
    "    extra filters for Cateogory 2.\n",
    "    \"\"\"\n",
    "    t = nl.lower()\n",
    "    wh = []\n",
    "    if \"english\" in t:\n",
    "        wh.append((\"languagecode\", \"en\"))\n",
    "    return wh\n",
    "\n",
    "# Establishing a set of rules the model must follow, created based on previous model struggles in SQL generation\n",
    "def rulescat2(nl: str, concept_id: str | None) -> list[str]:\n",
    "    \"\"\" The following function enforces rules the model must follow for category 2 SQL generation.\n",
    "        This was created to guide the model in SQL generation, since the same errors were continously observed\n",
    "        in SQl ouptuts from the model \"\"\"\n",
    "    muss = [\n",
    "        \"Only use table: TRUD_descriptiontable\",\n",
    "        \"Only use columns: conceptid, term, languagecode, typeid\",\n",
    "        \"Return exactly one SELECT statement (no comments)\",\n",
    "        \"Do not use JOINs or other tables\",\n",
    "    ]\n",
    "    # Encforcing rules on conceptid: A common struggle observed when SQL was created\n",
    "    if concept_id:\n",
    "        muss.append(\"Concept IDs are TEXT: wrap them in single quotes\")\n",
    "        muss.append(f\"Must use conceptid = '{concept_id}' exactly (do not change digits)\")\n",
    "    # Applyting language rule only if requested in NL\n",
    "    muss += [f\"Must include {col} = '{val}'\" for col, val in cat2filters(nl)]\n",
    "    return muss\n",
    "\n",
    "\n",
    "#Function that enforces category-2 sepcific rules on the model\n",
    "def violates_cat2(sql: str, nl: str, concept_id: str) -> str:\n",
    "    \"\"\"The following function verifies if any of the aformentioned rules were\n",
    "      violated during SQL generation\"\"\"\n",
    "    s = sql.strip()\n",
    "    lo = s.lower()\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s):\n",
    "        return \"Not a SELECT\"\n",
    "    if \"trud_relationshiptable\" in lo:\n",
    "        return \"Used TRUD_relationshiptable\"\n",
    "    if \"trud_descriptiontable\" not in lo:\n",
    "        return \"Missing TRUD_descriptiontable\"\n",
    "    #checking for hallucinated or forbidden columns\n",
    "    badcols = {\n",
    "        \"destinationid\", \"sourceid\", \"relationid\", \"productid\", \"prescriptionid\",\n",
    "        \"termid\", \"conjectid\", \"concertid\", \"label\", \"columns\", \"chart\"\n",
    "    }\n",
    "    for c in badcols:\n",
    "        if re.search(rf\"\\b{re.escape(c)}\\b\", lo):\n",
    "            return f\"Used non-existent/forbidden column '{c}'\"\n",
    "    # Encorocing the requirement that concept ID must match exactly if specified\n",
    "    if concept_id and not re.search(rf\"conceptid\\s*=\\s*'{re.escape(concept_id)}'\", s, flags=re.I):\n",
    "        return \"Missing or mutated conceptid predicate\"\n",
    "    #Including the expected filters like languagecode if present in NL\n",
    "    for col, val in cat2filters(nl):\n",
    "        if not re.search(rf\"\\b{re.escape(col)}\\s*=\\s*'{re.escape(val)}'\", s, flags=re.I):\n",
    "            return f\"Missing required predicate {col} = '{val}'\"\n",
    "    if not re.search(r\"(?is)^\\s*select\\s+(distinct\\s+)?term\\b\", s):\n",
    "        return \"Must select 'term'\"\n",
    "    return \"\"\n",
    "\n",
    "#Function formatting the examples into the required format\n",
    "def buildfewpromptcat2(eg, cuestion, schematxt, concept_id, k=3):\n",
    "    \"\"\" The following function formats the emxamples into the valid few-shot prompt format \"\"\"\n",
    "    rul = \"\\n- \".join(rulescat2(cuestion, concept_id))\n",
    "    dem = \"\\n\\n\".join([f\"query for: {q}\\nSQL: {sql.strip()}\" for q, sql in eg[:k]])\n",
    "    q = cuestion.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{s_hint}\\n\\nRules:\\n- {rul}\\n\\n\"\n",
    "        f\"tables:\\n{schematxt}\\n\\n\"\n",
    "        f\"Here are some examples:\\n{dem}\\n\\n\"\n",
    "        f\"Now query for: {q}?\\nSQL:\"\n",
    "    )\n",
    "\n",
    "#Schema hint\n",
    "s_hint = (\n",
    "    \"You write SQLite SQL.\\n\"\n",
    "    \"Return exactly one SELECT statement. No comments.\\n\"\n",
    "    \"Use only the tables and columns shown. Do not invent tables or columns.\")\n",
    "\n",
    "#Function that specifies \"bad\" words\n",
    "def badwords2(tokeniser):\n",
    "    \"\"\"\n",
    "    The following function aids the model in preventing hallucation, by forbidding the model to generate certain coulumns\n",
    "    names, based on observations in previous attempts.\n",
    "    \"\"\"\n",
    "    ban = [\n",
    "        \"destinationid\", \"sourceid\", \"relationid\", \"productid\", \"prescriptionid\",\n",
    "        \"termid\", \"conjectid\", \"concertid\", \"columns\", \"chart\"\n",
    "    ]\n",
    "    ids = []\n",
    "    for w in ban:\n",
    "        t = tokeniser(w, add_special_tokens=False).input_ids\n",
    "        if len(t) == 1:\n",
    "            ids.append(t)\n",
    "    return ids\n",
    "Bd_w= badwords2(tokeniser)\n",
    "\n",
    "#Function for generation of SQL queries with max 2 retries\n",
    "def generatewithretry(prompt, nl, concept_id, maxtries=2):\n",
    "    \"\"\"\n",
    "    The following function generates an SQL query from a natural language prompt using a language model,\n",
    "    and retries generation up to `maxtries` times if the output is still invalid\n",
    "    \"\"\"\n",
    "    lastsql, lastreason = \"\", \"\"\n",
    "    p = prompt\n",
    "    for _ in range(maxtries):\n",
    "        sql = extractsql(generatesqldecoderul(p))\n",
    "        reas = violates_cat2(sql, nl, concept_id)\n",
    "        good, whynot = valexecut(sql, conn_eval)\n",
    "        if not reas and good:\n",
    "            return sql, \"good\"\n",
    "        lastsql, lastreason = sql, reas or whynot\n",
    "        p += f\"\\n\\n previous output invalid: {lastreason}. Obey all Rules strictly.\"\n",
    "    return lastsql, f\"failed despite retries: {lastreason}\"\n",
    "\n",
    "#Specifying the schema table\n",
    "schem=(\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT);\")\n",
    "\n",
    "#empty list for the resuttls\n",
    "rfew=[]\n",
    "\n",
    "#Looping through category 2 via few-shot prompting: generating results\n",
    "for i, (q, expected_sql) in enumerate(sql_promts_category2):\n",
    "    concept_id =None\n",
    "    examples = [p for j, p in enumerate(sql_promts_category2) if j != i][:3]\n",
    "    promptfs = buildfewpromptcat2(examples, q, schem, concept_id, k=3)\n",
    "    gensql, status = generatewithretry(promptfs, q, concept_id, maxtries=2)\n",
    "    rfew.append({\n",
    "        \"Description\": q,\n",
    "        \"Prompt\": promptfs,\n",
    "        \"Generated sql\": gensql,\n",
    "        \"Expected sql\": expected_sql.strip(),\n",
    "        \"Status\": status\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt2 = pd.DataFrame(rfew)\n",
    "dt2 = dt2.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt2[\"exact match\"] = dt2.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt2[\"Sequence match (ASMS)\"] = dt2.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt2[\"EMA\"] = dt2.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"Exact Match Accuracy (EMA): {dt2['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt2['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt2['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores\n",
    "dt2_dc     = add_datacompy_scores(dt2, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt2_dcprec = add_datacompy_scores(dt2, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "dt2_dcrec  = add_datacompy_scores(dt2, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "dt2_dccol  = add_datacompy_scores(dt2, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt2_exc = tagexpairs(dt2, conn_eval)\n",
    "dt2_exc = add_sqlglot_equiv(dt2_exc, execo=True)\n",
    "mask = dt2_exc[\"sqlglot_equiv\"].notna()\n",
    "\n",
    "#Printing the datacompy, sQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence: {dt2_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of queries that executed: {mask.sum()} / {len(dt2_exc)}\")\n",
    "print(\"Datacompy (rows, F1):\", dt2_dc[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy (rows, precision):\", dt2_dcprec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy (rows, recall):\", dt2_dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy (columns, F1):\", dt2_dccol[\"datacompy_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XDLBFemISndD",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the category 2 resutls into a csv file\n",
    "dt2.to_csv(\"categoy2results.csv\", index=False)\n",
    "#Downloading the results for cateogry 1 into the local device: This specifically for google Collab\n",
    "files.download(\"categoy2results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U7zUVtorDcNW",
   "metadata": {
    "id": "U7zUVtorDcNW"
   },
   "source": [
    "Section 6.3 Few-shot prompting for Category 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiXGIszwyDoP",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the schema hint\n",
    "s_hint = (\n",
    "    \"You write SQLite SQL.\\n\"\n",
    "    \"Return exactly one SELECT statement. No comments.\\n\"\n",
    "    \"Use only the tables and columns shown. Do not invent tables or columns.\")\n",
    "\n",
    "#Defining filters applied to category 3 later for SQL generation\n",
    "def cat3filters(nl: str, term_text: str):\n",
    "    \"\"\" Required predicates for Cat3: exact term match, and language if mentioned.\n",
    "     Returns list of (column, value) pairs.\"\"\"\n",
    "    wh = [(\"term\", term_text)]\n",
    "    if \"english\" in nl.lower():\n",
    "        wh.append((\"languagecode\", \"en\"))\n",
    "    return wh\n",
    "\n",
    "#Function guides the model in deciding whuich columns to SELECT based on the NL questio\n",
    "def cat3selecttarget(nl: str) -> str:\n",
    "    \"\"\" The follwoing function guides the model in deciding which column to SELECT based on the NL question:\n",
    "        This was created to guide the model in SQL generation, since the same errors were continously observed\n",
    "        in SQl ouptuts from the model\n",
    "    \"\"\"\n",
    "    t = nl.lower()\n",
    "    if \"conceptid\" in t or \"concept id\" in t or \"concept-id\" in t:\n",
    "        return \"conceptid\"\n",
    "    # if the NL asks about synonym/preferred term or 'type', return typeid\n",
    "    if (\"synonym\" in t or \"preferred\" in t or \"typeid\" in t or\n",
    "        re.search(r\"\\btype\\b\", t)):\n",
    "        return \"typeid\"\n",
    "    #Specfying the default\n",
    "    return \"typeid\"\n",
    "\n",
    "#Establishing a set of rules the model must follow, created based on previous model struggles in SQL generation\n",
    "def rulescat3(nl: str, term_text: str | None) -> list[str]:\n",
    "    \"\"\" The following function enforces rules the model must follow for category 1 SQL generation.\n",
    "        This was created to guide the model in SQL generation, since the same errors were continously observed\n",
    "        in SQl ouptuts from the model\n",
    "    \"\"\"\n",
    "    targetcol = cat3selecttarget(nl)\n",
    "    muss = [\n",
    "        \"Only use table: TRUD_descriptiontable\",\n",
    "        \"Only use columns: conceptid, term, typeid, languagecode\",\n",
    "        \"Return exactly one SELECT statement (no comments)\",\n",
    "        f\"Must SELECT only the column: {targetcol}\",\n",
    "        \"Do not use JOINs or other tables\",\n",
    "        \"Concept and type IDs are TEXT: wrap string literals in single quotes\",\n",
    "    ]\n",
    "    #Encforcing rules on conceptid: A common struggle observed when SQL was created\n",
    "    if term_text:  # only constrain if we truly got it from the NL\n",
    "        muss.append(f\"Must include term = '{term_text}' exactly (no LIKE, no partial matches)\")\n",
    "    if \"english\" in nl.lower():\n",
    "        muss.append(\"Must include languagecode = 'en'\")\n",
    "    return muss\n",
    "\n",
    "#Functions verifies if any of the aformentioned rules from rulescat3\n",
    "def violates_cat3(sql: str, nl: str, term_text: str | None) -> str:\n",
    "    \"\"\"The following function verifies if any of the aformentioned rules were\n",
    "       violated during SQL generation\n",
    "    \"\"\"\n",
    "    s = sql.strip()\n",
    "    lo = s.lower()\n",
    "    # Select must apear in the SQL ouputs\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s): return \"Not a SELECT\"\n",
    "    #The correct must be specificied in the SQL ouputs\n",
    "    if \"trud_relationshiptable\" in lo: return \"Used TRUD_relationshiptable\"\n",
    "    if \"trud_descriptiontable\" not in lo: return \"Missing TRUD_descriptiontable\"\n",
    "    # Preveting hallucinated columns\n",
    "    badcols={\"destinationid\",\"sourceid\",\"relationid\",\"productid\",\"prescriptionid\",\"termid\",\"conjectid\",\"concertid\",\"columns\",\"chart\"}\n",
    "    for c in badcols:\n",
    "        if re.search(rf\"\\b{re.escape(c)}\\b\", lo):\n",
    "            return f\"Used non-existent/forbidden column '{c}'\"\n",
    "    m = re.search(r\"(?is)^\\s*select\\s+(.*?)\\s+from\\s+trud_descriptiontable\", s)\n",
    "    if not m: return \"Malformed SELECT or FROM\"\n",
    "    target_col=cat3selecttarget(nl)\n",
    "    select_list=re.sub(r\"\\s\", \"\", m.group(1).lower())\n",
    "    if select_list not in {target_col, f\"distinct{target_col}\"}:\n",
    "        return f\"Must select only '{target_col}'\"\n",
    "    if term_text:\n",
    "        term_escaped = re.escape(term_text)\n",
    "        if not re.search(rf\"\\bterm\\s*=\\s*'{term_escaped}'\\b\", s, flags=re.I):\n",
    "            return f\"Missing required predicate term = '{term_text}'\"\n",
    "    if \"english\" in nl.lower() and not re.search(r\"\\blanguagecode\\s*=\\s*'en'\\b\", s, flags=re.I):\n",
    "        return \"Missing required predicate languagecode = 'en'\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "#Function generates an SQL query from a natural language prompt using a language mode, retrying 2 times\n",
    "def generatewithretry(prompt, nl, concept_id, maxtries=2):\n",
    "    \"\"\"The following function generates an SQL query from a natural language prompt using a language model,\n",
    "       and retries generation up to `maxtries` times if the output is still invalid\n",
    "    \"\"\"\n",
    "    lastsql,lastreason = \"\", \"\"\n",
    "    p = prompt\n",
    "    for _ in range(maxtries):\n",
    "        sql = extractsql(generatesqldecoderul(p))\n",
    "        reas = violates_cat3(sql, nl, concept_id)\n",
    "        good, whynot = valexecut(sql, conn_eval)\n",
    "        if not reas and good:\n",
    "            return sql, \"good\"\n",
    "        lastsql, lastreason= sql, reas or whynot\n",
    "        p += f\"\\n\\n previous output invalid: {lastreason}. Obey all Rules strictly.\"\n",
    "    return lastsql, f\"failed depsite retries: {lastreason}\"\n",
    "\n",
    "#Function the emxamples into the valid few-shot prompt format\n",
    "def buildfewpromptcat3(eg, cuestion, schematxt, concept_id, term_text, k=3):\n",
    "    \"\"\" The following function formats the emxamples into the valid few-shot prompt format \"\"\"\n",
    "    rul = \"\\n- \".join(rulescat3(cuestion, term_text))\n",
    "    dem = \"\\n\\n\".join([f\"query for: {q}\\nSQL: {sql.strip()}\" for q, sql in eg[:k]])\n",
    "    q = cuestion.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{s_hint}\\n\\nRules:\\n- {rul}\\n\\n\"\n",
    "        f\"tables:\\n{schematxt}\\n\\n\"\n",
    "        f\"Here are some examples:\\n{dem}\\n\\n\"\n",
    "        f\"Now query for: {q}?\\nSQL:\"\n",
    "    )\n",
    "\n",
    "#Definfing a function that aids in preventing hallucation,by forbidding the model to generate certain coulumns\n",
    "def badwords3(tokeniser):\n",
    "    \"\"\" The following function aids the model in preventing hallucation, by forbidding the model to generate certain coulumns\n",
    "        names: based on observations in previous attempts. \"\"\"\n",
    "    ban =[\"destinationid\",\"sourceid\",\"relationid\",\"productid\",\"prescriptionid\",\"termid\",\"conjectid\",\"concertid\",\"columns\",\"chart\"]\n",
    "    ids =[]\n",
    "    for w in ban:\n",
    "        t =tokeniser(w, add_special_tokens=False).input_ids\n",
    "        if len(t) ==1: ids.append(t)\n",
    "    return ids\n",
    "\n",
    "#Applying the function for preventative hallucination\n",
    "Bd_w= badwords3(tokeniser)\n",
    "schem = (\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT);\")\n",
    "#empty list for the resuttls\n",
    "rfew = []\n",
    "\n",
    "#Looping through category 3 via few-shot prompting: generating results\n",
    "for i, (q, expected_sql) in enumerate(sql_promts_category3):\n",
    "    term_text = None\n",
    "    #Specfiying the number of examples the model is show (K), excluding the current item\n",
    "    examples = [p for j, p in enumerate(sql_promts_category3) if j != i][:3]\n",
    "    # add the missing term_text arg\n",
    "    promptfs = buildfewpromptcat3(examples, q, schem, concept_id, term_text, k=3)\n",
    "    # generatewithretry's 3rd param is used by violates_cat3 as the term — pass term_text here\n",
    "    gensql, status = generatewithretry(promptfs, q, term_text, maxtries=2)\n",
    "    rfew.append({\n",
    "    \"Description\": q,\n",
    "    \"Prompt\": promptfs,\n",
    "    \"Generated sql\": gensql,\n",
    "    \"Expected sql\": expected_sql.strip(),\n",
    "    \"Status\": status})\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt3 = pd.DataFrame(rfew)\n",
    "dt3 = dt3.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt3[\"exact match\"] = dt3.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt3[\"Sequence match (ASMS)\"] = dt3.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt3[\"EMA\"] = dt3.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {dt3['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt3['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt3['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores,\n",
    "dt3_dc =add_datacompy_scores(dt3, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt3dcprec = add_datacompy_scores(dt3, conn_eval, mode=\"rows\",metric=\"precision\")\n",
    "dt3dcrec  = add_datacompy_scores(dt3, conn_eval, mode=\"rows\",metric=\"recall\")\n",
    "dt3dccol  = add_datacompy_scores(dt3, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt3_exc = tagexpairs(dt3, conn_eval)\n",
    "dt3_exc= add_sqlglot_equiv(dt3_exc, execo=True)\n",
    "mask = dt3_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {dt3_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(dt3_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", dt3_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\",dt3dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", dt3dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", dt3dccol[\"datacompy_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_PT1vCnBhOEG",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the category 3 resutls into a csv file\n",
    "dt3.to_csv(\"categoy3results.csv\", index=False)\n",
    "#Downloading the results for cateogry 1 into the local device: This specifically for google Collab\n",
    "files.download(\"categoy3results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o19iESRIGpQ6",
   "metadata": {
    "id": "o19iESRIGpQ6"
   },
   "source": [
    "Section 6.4 Few-shot prompting for Category 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zam2ZLAC9TC4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the schema and hint\n",
    "sc = \"\"\"CREATE TABLE TRUD_relationshiptable ( sourceid TEXT, destinationid TEXT, typeid TEXT);\"\"\"\n",
    "sh = (\n",
    "    \"You write SQLite SQL.\\n\"\n",
    "    \"Return exactly one SELECT statement. No comments.\\n\"\n",
    "    \"Use only the tables and columns shown. Do not invent tables or columns.\")\n",
    "\n",
    "#Function encorcing category-4-specific reules\n",
    "def cat4filters(nl: str, concept_id: str):\n",
    "    \"\"\"\n",
    "    The following function enforces category-4-pecific SQL rules by verifying\n",
    "    whether any of the defined requirements are violated in the generated SQL.\n",
    "    This was added after due to repearted allucination patterns were observed in category-4 prompts.\n",
    "    In the case that any rule is broken, a descriptive string is returned specifying what went wrong.\n",
    "    If all rules pass, an empty string \"\" is returned, indicating that the SQL is valid.\n",
    "    \"\"\"\n",
    "    t =nl.lower()\n",
    "    p= []\n",
    "    if \"parent\" in t or \"is a\" in t or \"broader\" in t:\n",
    "        p.append((\"sourceid\", concept_id))\n",
    "        proj = [\"destinationid\"]\n",
    "        dir = \"up\"\n",
    "    elif \"child\" in t or \"descendant\" in t or \"narrower\" in t:\n",
    "        p.append((\"destinationid\", concept_id))\n",
    "        proj = [\"sourceid\"]\n",
    "        dir = \"down\"\n",
    "    else:\n",
    "        #Showing both sides constrained by concept_id anywhere\n",
    "        p.append((\"sourceid\", concept_id))\n",
    "        p.append((\"destinationid\", concept_id))\n",
    "        proj=[\"sourceid\", \"destinationid\"]\n",
    "        dir = \"either\"\n",
    "    # In the case that NL contains an explicit numeric typeid, it needs to be inlcuded\n",
    "    m = re.search(r\"\\b(900\\d{12}|116680003|\\d{6,})\\b\", t)\n",
    "    if m:\n",
    "        p.append((\"typeid\", m.group(1)))\n",
    "    return dir, p, proj\n",
    "\n",
    "#Function defining category 4-sepcifc rules\n",
    "def rule_cat4(nl: str, concept_id: str) -> tuple[list[str], list[str]]:\n",
    "    \"\"\" Function defines category 4sepcifc rules\"\"\"\n",
    "    direction, preds, proj = cat4filters(nl, concept_id)\n",
    "    muss = [\n",
    "        \"Only use table: TRUD_relationshiptable\",\n",
    "        \"Only use columns: sourceid, destinationid, typeid\",\n",
    "        f\"Must SELECT only: {', '.join(proj)}\",\n",
    "        \"Return exactly one SELECT statement (no comments)\",\n",
    "    ]\n",
    "    for col, val in preds:\n",
    "        muss.append(f\"Must include predicate {col} = '{val}'\")\n",
    "    return muss, proj\n",
    "\n",
    "#Function formating category 4 examples into the valid prompt format\n",
    "def buildpromptcat4(eg, cuq, concept_id, k=3):\n",
    "    \"\"\" The following function formats the emxamples into the valid few-shot prompt format \"\"\"\n",
    "    direction, preds, projection = cat4filters(cuq, concept_id)\n",
    "    muss, projection = rule_cat4(cuq, concept_id)\n",
    "    rultxt = \"\\n- \".join(muss)\n",
    "    dem = \"\\n\\n\".join([f\"Q: {q}\\nSQL: {sql.strip()}\" for q, sql in eg[:k]])\n",
    "    q = cuq.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{sh}\\n\\n\"\n",
    "        f\"tables:\\n{sc}\\n\\n\"\n",
    "        f\"Examples:\\n{dem}\\n\\n\"\n",
    "        f\"Rules:\\n- {rultxt}\\n\\n\"\n",
    "        f\"Q: {q}?\\nSQL:\")\n",
    "\n",
    "#Function preventing column hallucination by specifying particular \"bad\" words the model is not allowed to use\n",
    "def badwordscat4(tokeniser):\n",
    "    \"\"\"Function specifying \"bad words\" the model is forbidden as column nuames, as a perventative measure\"\"\"\n",
    "    b = [\"column\",\"columns\",\"chart\",\"table\",\"label\",\"description\",\"name\",\"conceptid\",\"term\",\"languagecode\"]\n",
    "    ids=[];\n",
    "    for w in b:\n",
    "        t = tokeniser(w, add_special_tokens=False).input_ids\n",
    "        if len(t)==1: ids.append(t)\n",
    "    return ids\n",
    "#Applyting the bad words function\n",
    "bd_4 = badwordscat4(tokeniser)\n",
    "\n",
    "#Functions verifies if any of the aformentioned rules from rulescat3\n",
    "def violates_cat4(sql: str, nl: str, concept_id: str) -> str:\n",
    "    \"\"\"The following function verifies if any of the aformentioned rules were\n",
    "      violated during SQL generation\n",
    "    \"\"\"\n",
    "    s = sql.strip(); lo = s.lower()\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s): return \"Not a SELECT\"\n",
    "    if \"trud_relationshiptable\" not in lo: return \"Missing TRUD_relationshiptable\"\n",
    "    bd = {\"conceptid\",\"term\",\"languagecode\"}\n",
    "    if any(re.search(rf\"\\b{c}\\b\", lo) for c in bd): return \"Used columns from description table\"\n",
    "    if concept_id and not re.search(rf\"(sourceid|destinationid)\\s*=\\s*'{re.escape(concept_id)}'\", s, flags=re.I):\n",
    "        return \"Missing concept ID in sourceid/destinationid\"\n",
    "    bad_proj = re.search(r\"(?is)^\\s*select\\s+(distinct\\s+)?(.*?)\\s+from\", s)\n",
    "    if bad_proj:\n",
    "        proj_list = bad_proj.group(2).lower()\n",
    "        for tok in re.split(r\"\\s*,\\s*\", proj_list):\n",
    "            base = re.sub(r\"\\s+as\\s+.*$\", \"\", tok.strip())\n",
    "            if base not in {\"sourceid\",\"destinationid\",\"typeid\",\"*\"}:\n",
    "                return f\"Projected invalid column: {base}\"\n",
    "    return \"\"\n",
    "\n",
    "#Defining the empty lists\n",
    "rfew = []\n",
    "\n",
    "#Looping through categroy 4, generating SQL\n",
    "for i, (q, expected_sql) in enumerate(sql_prompts_category4):\n",
    "    concept_id = None\n",
    "    # Building filtered few-shot examples\n",
    "    examples = []\n",
    "    for j, p in enumerate(sql_prompts_category4):\n",
    "        if j == i:\n",
    "            continue\n",
    "        # Skipping any example of the same concept_id in its SQL\n",
    "        if concept_id and re.search(rf\"'{re.escape(concept_id)}'\", p[1]):\n",
    "            continue\n",
    "        examples.append(p)\n",
    "    examples = examples[:3]  # keep only first 3\n",
    "    #Building the few shot prompt\n",
    "    prompt_fs = buildpromptcat4(examples, q, concept_id, k=3)\n",
    "    #Creatigns the category 4 bans\n",
    "    inp = tokeniser(prompt_fs, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inp,\n",
    "            num_beams=nr_bm,\n",
    "            max_new_tokens=max_tok,\n",
    "            no_repeat_ngram_size= np_ngram,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokeniser.pad_token_id,\n",
    "            bad_words_ids=bd_4 ,\n",
    "            do_sample=False,)\n",
    "    gensql = extractsql(tokeniser.decode(out_ids[0], skip_special_tokens=True))\n",
    "    #Adding the guardrails within the loop itself\n",
    "    reas = violates_cat4(gensql, q, concept_id)\n",
    "    good, why = valexecut(gensql, conn_eval)\n",
    "    status = \"ok\" if (reas == \"\" and good) else f\"failed: {reas or why}\"\n",
    "    rfew.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt_fs,\n",
    "        \"generated_sql\": gensql,\n",
    "        \"expected_sql\": expected_sql.strip(),\n",
    "        \"status\": status\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt4 = pd.DataFrame(rfew)\n",
    "dt4 = dt4.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt4[\"exact match\"] = dt4.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt4[\"Sequence match (ASMS)\"] = dt4.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt4[\"EMA\"] = dt4.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accurcy Accuracy:{dt4['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt4['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt4['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores,\n",
    "dt4_dc = add_datacompy_scores(dt4, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt4dcprec = add_datacompy_scores(dt4, conn_eval, mode=\"rows\",metric=\"precision\")\n",
    "dt4dcrec  = add_datacompy_scores(dt4, conn_eval, mode=\"rows\",metric=\"recall\")\n",
    "dt4dccol  = add_datacompy_scores(dt4, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt4_exc = tagexpairs(dt4, conn_eval)\n",
    "dt4_exc= add_sqlglot_equiv(dt4_exc, execo=True)\n",
    "mask = dt4_exc[\"sqlglot_equiv\"].notna()\n",
    "\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {dt4_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(dt4_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", dt4_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", dt4dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", dt4dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", dt4dccol[\"datacompy_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NrQp-9OaUuRP",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving category 4 results into a CSV fromat\n",
    "dt4.to_csv(\"category4_results.csv\", index=False)\n",
    "#Downloading the CSV file to the local device\n",
    "files.download(\"category4_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cRkD76nKdI",
   "metadata": {
    "id": "16cRkD76nKdI"
   },
   "source": [
    "Section 6.5 Few-shot prompting for Category 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fSpk_-Dfiu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as _re\n",
    "#Respecifying the schema hint\n",
    "s_hint = (\n",
    "    \"You write SQLite SQL.\\n\"\n",
    "    \"Return exactly one SELECT statement. No comments.\\n\"\n",
    "    \"Use only the tables and columns shown. Do not invent tables or columns.\")\n",
    "\n",
    "#Function generating a SQL query from a natural language prompt using a guarded decoding strategy\n",
    "def gensqlguarded(p):\n",
    "    \"\"\"The following function generates a SQL query from a NLprompt using beam search with guardrails.\n",
    "      Thereby tokenising the input prompt and passes it through a model (T5)\n",
    "      to generate a SQL query, using beam search to improve generation quality and include\n",
    "      constraints to avoid unsafe or repetitive SQL patterns.\"\"\"\n",
    "    ip= tokeniser(p, padding=False, truncation=False, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "      out_ids = model.generate(**ip, num_beams=nr_bm, max_new_tokens=max_tok,\n",
    "                               no_repeat_ngram_size=np_ngram, early_stopping=True,\n",
    "                               pad_token_id=tokeniser.pad_token_id,\n",
    "                               bad_words_ids=bd5_,\n",
    "                               do_sample=False,)\n",
    "      return tokeniser.decode(out_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "#Function, serving as preventative measure for hallucination by forbidding the generation of certain column names\n",
    "def bd5words(tok):\n",
    "    \"\"\" The following function aids the model in preveting hallucatoin, by forbidding the model to generate certain coulumns\n",
    "         names: based on observations in previous attempts.\n",
    "    \"\"\"\n",
    "    b =[\"column\",\"columns\",\"chart\",\"table\",\"destinationid\",\"sourceid\",\"relationid\",\n",
    "           \"productid\",\"prescriptionid\",\"termid\",\"conjectid\",\"concertid\",\"label\",\"description\",\"name\"]\n",
    "    ids=[]\n",
    "    for w in b:\n",
    "        t = tok(w, add_special_tokens=False).input_ids\n",
    "        if len(t) ==1: ids.append(t)\n",
    "    return ids\n",
    "bd5_ = bd5words(tokeniser)\n",
    "\n",
    "#Establishing a set of rules the model must follow, created based on previous model struggles in SQL generation\n",
    "def rules5(nl: str, concept_id: str) -> list[str]:\n",
    "    \"\"\" The following function enforces rules the model must follow for category 5 SQL generation.\n",
    "      This was created to guide the model in SQL generation, since the same errors were continously observed\n",
    "      in SQl ouptuts from the model \"\"\"\n",
    "    p = cat5pl(nl)\n",
    "    muss =[\n",
    "        \"Only use table: TRUD_descriptiontable\",\n",
    "        \"Only use columns: conceptid, term, typeid, languagecode\",\n",
    "        \"Concept IDs are TEXT: wrap them in single quotes\",\n",
    "        # f\"Must use conceptid = '{concept_id}' exactly (do not change digits)\",\n",
    "    ]\n",
    "    for col, val in p[\"filters_extra\"]:\n",
    "        muss.append(f\"Include {col} = '{val}'\")\n",
    "    #providign a projection hint due to previous struggles\n",
    "    if p[\"agg\"]:\n",
    "        muss.append(\"Return a single aggregation row (COUNT)\")\n",
    "    elif p[\"distinct\"]:\n",
    "        muss.append(\"Select DISTINCT term only\")\n",
    "    else:\n",
    "        muss.append(\"Select term only\")\n",
    "    return muss\n",
    "\n",
    "# Function generates an SQL query from a natural language with 2 retries\n",
    "def generatewithretry(prompt, nl, concept_id, maxtries=2):\n",
    "    \"\"\"The following function generates an SQL query from a natural language prompt using a language model,\n",
    "      and retries generation up to `maxtries` times if the output is still invalid\n",
    "     \"\"\"\n",
    "    lastsql,lastreason = \"\", \"\"\n",
    "    p = prompt\n",
    "    for _ in range(maxtries):\n",
    "        sql = extractsql(generatesqldecoderul(p))\n",
    "        reas = violates_cat5(sql, nl, concept_id)\n",
    "        good, whynot = valexecut(sql, conn_eval)\n",
    "        if not reas and good:\n",
    "            return sql, \"good\"\n",
    "        lastsql, lastreason= sql, reas or whynot\n",
    "        p += f\"\\n\\n previous output invalid: {lastreason}. Obey all Rules strictly.\"\n",
    "    return lastsql, f\"failed depsite retries: {lastreason}\"\n",
    "\n",
    "# function determines how to query Cat5 concepts based on the natural language input\n",
    "def cat5pl(nl: str):\n",
    "    \"\"\"\n",
    "    Infers the SQL projection and necessary filters for a Cat5 term query based on input language.\n",
    "    The heuristics applied were:\n",
    "      - Mentions of 'count' trigger COUNT(*)\n",
    "      - Phrases like 'all', 'list', or 'descriptions' indicate term listings (possibly DISTINCT)\n",
    "      - Language/type hints like 'preferred', 'fsn', or 'english' map to specific filters\n",
    "      - Default fallback is to list terms tied to the conceptid\n",
    "    \"\"\"\n",
    "    t = nl.lower()\n",
    "    pln = {\n",
    "        \"projection\": [\"term\"],\n",
    "        \"distinct\": False,\n",
    "        \"agg\": None,\n",
    "        \"require_typeid\": False,\n",
    "        \"filters_extra\": []}\n",
    "    if \"count\" in t:\n",
    "        pln[\"agg\"] = \"COUNT(*) AS cnt\"\n",
    "        pln[\"projection\"] = []\n",
    "    else:\n",
    "        if \"distinct\" in t or \"unique\" in t:\n",
    "            pln[\"distinct\"] = True\n",
    "        if \"show\" in t or \"list\" in t or \"all\" in t or \"descriptions\" in t:\n",
    "            pln[\"projection\"] = [\"term\"]\n",
    "    #enforcing hints for langauge specicif identifiers\n",
    "    if \"english\" in t:\n",
    "        pln[\"filters_extra\"].append((\"languagecode\", \"en\"))\n",
    "\n",
    "    if \"full name\" in t or \"fsn\" in t:\n",
    "        pln[\"filters_extra\"].append((\"typeid\", FSN))\n",
    "        pln[\"require_typeid\"] = True\n",
    "    elif \"preferred\" in t or \"synonym\" in t:\n",
    "        pln[\"filters_extra\"].append((\"typeid\", SYN))\n",
    "        pln[\"require_typeid\"] = True\n",
    "    return pln\n",
    "\n",
    "#Function formating the examples into the accurate prompt format\n",
    "def buildfewpromptcat5(eg, cuestion, schematxt, concept_id, k=3):\n",
    "    \"\"\" The following function formats the emxamples into the valid few-shot prompt format \"\"\"\n",
    "    rul = \"\\n- \".join(rulescat1(cuestion, concept_id))\n",
    "    dem = \"\\n\\n\".join([f\"query for: {q}\\nSQL: {sql.strip()}\" for q, sql in eg[:k]])\n",
    "    q = cuestion.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{s_hint}\\n\\nRules:\\n- {rul}\\n\\n\"\n",
    "        f\"tables:\\n{schematxt}\\n\\n\"\n",
    "        f\"Here are some examples:\\n{dem}\\n\\n\"\n",
    "        f\"Now query for: {q}?\\nSQL:\"\n",
    "    )\n",
    "\n",
    "#Function flagging if any for the aforementioned rules are not adhered to\n",
    "def violates_cat5(sql: str, nl: str, concept_id: str) -> str:\n",
    "    \"\"\"The following function verifies if any of the aformentioned rules were\n",
    "      violated during SQL generation\"\"\"\n",
    "    s = sql.strip(); lo = s.lower()\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s): return \"Not a SELECT\"\n",
    "    if \"trud_descriptiontable\" not in lo: return \"Missing TRUD_descriptiontable\"\n",
    "    #Forbidding relationship cols; allow only desc cols in predicates\n",
    "    forbidden = {\"sourceid\",\"destinationid\",\"relationid\",\"productid\",\"prescriptionid\",\n",
    "                 \"termid\",\"conjectid\",\"concertid\",\"label\",\"description\",\"column\",\"columns\",\"chart\",\"table\",\"name\"}\n",
    "    if any(_re.search(rf\"\\b{c}\\b\", lo) for c in forbidden):\n",
    "        return \"Used non-existent/forbidden column\"\n",
    "    if concept_id and not _re.search(rf\"conceptid\\s*=\\s*'{re.escape(concept_id)}'\", s, flags=re.I):\n",
    "        return \"Missing or mutated conceptid predicate\"\n",
    "    #cecking plan-derived expectations (soft but enforced)\n",
    "    plan = cat5pl(nl)\n",
    "    if plan[\"agg\"]:\n",
    "        if \"count(\" not in lo:\n",
    "            return \"Expected COUNT aggregation\"\n",
    "    else:\n",
    "        #the proejction must entail distinct\n",
    "        m = _re.match(r\"(?is)^\\s*select\\s+(distinct\\s+)?(.+?)\\s+from\", s)\n",
    "        if not m: return \"Malformed SELECT\"\n",
    "        is_distinct = bool(m.group(1))\n",
    "        proj = [p.strip().lower() for p in m.group(2).split(\",\")]\n",
    "        if \"term\" not in [p.split()[-1] for p in proj]:  # ignore aliases\n",
    "            return \"Must include 'term' in projection\"\n",
    "        if plan[\"distinct\"] and not is_distinct:\n",
    "            return \"Expected DISTINCT in projection\"\n",
    "\n",
    "    # if NL explicitly asked fsn/syn, enforce presence of typeid filter; otherwise optional\n",
    "    if plan[\"require_typeid\"]:\n",
    "        needed = [(\"typeid\", v) for (c,v) in plan[\"filters_extra\"] if c == \"typeid\"]\n",
    "        for _, v in needed:\n",
    "            if not _re.search(rf\"\\btypeid\\s*=\\s*'{re.escape(v)}'\", s, flags=re.I):\n",
    "                return \"Missing required typeid filter\"\n",
    "    # language filter if present in NL\n",
    "    if any(c == \"languagecode\" for c,_ in plan[\"filters_extra\"]):\n",
    "        if not _re.search(r\"\\blanguagecode\\s*=\\s*'en'\", s, flags=re.I):\n",
    "            return \"Missing required languagecode='en'\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "#Specifying the schema\n",
    "schem = (\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT);\")\n",
    "#empty list for the resuttls\n",
    "rfew = []\n",
    "#\n",
    "concept_id = None\n",
    "\n",
    "#Looping through category 5 via few-shot prompting: generating results\n",
    "for i, (q, expected_sql) in enumerate(sql_prompts_category5):\n",
    "    term_text = None\n",
    "    #Specfiying the number of examples the model is show (K), excluding the current item\n",
    "    examples = [p for j,p in enumerate(sql_prompts_category5) if j != i][:3]\n",
    "    promptfs = buildfewpromptcat5(examples, q, schem, concept_id, k=3)\n",
    "    gensql, status = generatewithretry(promptfs, q, concept_id, maxtries=2)\n",
    "    rfew.append({\n",
    "        \"Description\": q,\n",
    "        \"Prompt\": promptfs,\n",
    "        \"Generated sql\": gensql,\n",
    "        \"Expected sql\": expected_sql.strip(),\n",
    "        \"Status\": status})\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt5 = pd.DataFrame(rfew)\n",
    "dt5 = dt5.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt5[\"exact match\"] = dt5.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt5[\"Sequence match (ASMS)\"] = dt5.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt5[\"EMA\"] = dt5.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {dt5['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt5['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt5['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "dt5_dc =add_datacompy_scores(dt5, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt5dcprec = add_datacompy_scores(dt5, conn_eval, mode=\"rows\",metric=\"precision\")\n",
    "dt5dcrec  = add_datacompy_scores(dt5, conn_eval, mode=\"rows\",metric=\"recall\")\n",
    "dt5dccol  = add_datacompy_scores(dt5, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt5_exc = tagexpairs(dt5, conn_eval)\n",
    "dt5_exc = add_sqlglot_equiv(dt5_exc, execo=True)\n",
    "mask = dt5_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {dt5_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(dt5_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", dt5_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\",dt5dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", dt5dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", dt5dccol[\"datacompy_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Mj0dRf4Vd2T",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Category 5 results to CSV with correct filename\n",
    "dt5.to_csv(\"category5_results.csv\", index=False)\n",
    "# Downloadoding the csv into local device\n",
    "files.download(\"category5_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KEc9y15L3pke",
   "metadata": {
    "id": "KEc9y15L3pke"
   },
   "source": [
    "Section 6.6 Few-shot prompting for Category 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ud9W-fil3w69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hJ1W8UdS-gwl",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function below specifies the filters applied to later SQL generation\n",
    "def cat6_filters(nl: str, module_id: str):\n",
    "    \"\"\" Required predicates for Cat6: moduleid must be exact; add language if requested.\n",
    "    Returns list of (column, value) pairs.\"\"\"\n",
    "    where = []\n",
    "    if module_id:\n",
    "        where.append((\"moduleid\", module_id))\n",
    "    if \"english\" in nl.lower():\n",
    "        where.append((\"languagecode\", \"en\"))\n",
    "    return where\n",
    "\n",
    "#function below provides the model with rules it must adhere to\n",
    "def rulescat6(nl: str, module_id: str) -> list[str]:\n",
    "    \"\"\"the following function provides the model with rules it must adhere to\"\"\"\n",
    "    musss = [\n",
    "        \"Only use table: TRUD_descriptiontable\",\n",
    "        \"Only use columns: term, moduleid, languagecode, conceptid, typeid\",\n",
    "        \"Return exactly one SELECT statement. No comments.\",\n",
    "        \"Do not use JOINs or other tables\",\n",
    "        \"String literals must be wrapped in single quotes\",\n",
    "        \"Must select only the column: term\"]\n",
    "        # f\"Must include moduleid = '{module_id}' exactly\"\n",
    "    if \"english\" in nl.lower():\n",
    "        musss.append(\"Must include languagecode = 'en'\")\n",
    "    return musss\n",
    "\n",
    "#Function below checks whether the rules are adhered to\n",
    "def  violates_cat6(sql: str, nl: str, module_id: str) -> str:\n",
    "    s = sql.strip()\n",
    "    lo = s.lower()\n",
    "    #SELECT must be specified in the query, additionallly from the correct table\n",
    "    if not re.match(r\"(?is)^\\s*select\\b\", s): return \"Not a SELECT\"\n",
    "    if \"trud_relationshiptable\" in lo: return \"Used TRUD_relationshiptable\"\n",
    "    if \"trud_descriptiontable\" not in lo: return \"Missing TRUD_descriptiontable\"\n",
    "    #Banning evident non existent columns\n",
    "    bdcol = {\"destinationid\",\"sourceid\",\"relationid\",\"productid\",\"prescriptionid\",\n",
    "                \"termid\",\"conjectid\",\"concertid\",\"columns\",\"chart\"}\n",
    "    for c in bdcol:\n",
    "        if re.search(rf\"\\b{re.escape(c)}\\b\", lo):\n",
    "            return f\"Used non-existent/forbidden column '{c}'\"\n",
    "    #Select must be specified, with Distinct also being allowed\n",
    "    m = re.search(r\"(?is)^\\s*select\\s+(.*?)\\s+from\\s+trud_descriptiontable\", s)\n",
    "    if not m:\n",
    "        return \"Malformed SELECT or FROM\"\n",
    "    select_list = re.sub(r\"\\s\", \"\", m.group(1).lower())\n",
    "    if select_list not in {\"term\", \"distinctterm\"}:\n",
    "        return \"Must select only 'term'\"\n",
    "    #The Queries msust inlcude the exact moduleid predicats\n",
    "    if module_id and not re.search(rf\"\\bmoduleid\\s*=\\s*'{re.escape(module_id)}'\\b\", s, flags=re.I):\n",
    "        return f\"Missing required predicate moduleid = '{module_id}'\"\n",
    "    #Specying langauge predicates for use if neccessary\n",
    "    if \"english\" in nl.lower():\n",
    "        if not re.search(r\"\\blanguagecode\\s*=\\s*'en'\\b\", s, flags=re.I):\n",
    "            return \"Missing required predicate languagecode = 'en'\"\n",
    "    return \"\"\n",
    "\n",
    "#The following function formats the emxamples into the valid few-shot prompt format\n",
    "def buildfewpromptcat6(examples, cq, schematxt, module_id, k=3):\n",
    "    \"\"\" The following function formats the emxamples into the valid few-shot prompt format \"\"\"\n",
    "    rul =\"\\n- \".join(rulescat6(cq, module_id))\n",
    "    de=\"\\n\\n\".join([f\"query for: {q}\\nSQL: {sql.strip()}\" for q, sql in examples[:k]])\n",
    "    q=cq.strip().rstrip(\"?\")\n",
    "    return (\n",
    "        f\"{s_hint}\\n\\nRules:\\n- {rul}\\n\\n\"\n",
    "        f\"tables:\\n{schematxt}\\n\\n\"\n",
    "        f\"Here are some examples:\\n{de}\\n\\n\"\n",
    "        f\"Now query for: {q}?\\nSQL:\"\n",
    "    )\n",
    "\n",
    "#The following function serves as a preventative measure for hallucination\n",
    "def badwords6(tok):\n",
    "    \"\"\" The following function aids the model in preveting hallucatoin, by forbidding the model to generate certain coulumns\n",
    "      names: based on observations in previous attempts.\n",
    "    \"\"\"\n",
    "    #Definign the common and inccorect column names that the model must not ouput\n",
    "    b = [\"destinationid\",\"sourceid\",\"relationid\",\"productid\",\"prescriptionid\",\"termid\",\"conjectid\",\"concertid\",\"columns\",\"chart\"]\n",
    "    ids = []\n",
    "    for w in b:\n",
    "        t = tok(w, add_special_tokens=False).input_ids\n",
    "        if len(t)==1:\n",
    "            ids.append(t)\n",
    "    return ids\n",
    "\n",
    "bd_6 = badwords6(tokeniser)\n",
    "\n",
    "#The following generates an SQL query from a natural language prompt, retrying 2 times\n",
    "def generatewithretry6(prompt, nl, module_id, maxtries=2):\n",
    "    \"\"\"The following function generates an SQL query from a natural language prompt using a language model,\n",
    "       and retries generation up to `maxtries` times if the output is still invalid\n",
    "    \"\"\"\n",
    "    lastsql,lastreason = \"\", \"\"\n",
    "    p = prompt\n",
    "    for _ in range(maxtries):\n",
    "        sql = extractsql(generatesqldecoderul(p))\n",
    "        reas = violates_cat6(sql, nl, module_id)\n",
    "        good, whynot = valexecut(sql, conn_eval)\n",
    "        if not reas and good:\n",
    "            return sql, \"good\"\n",
    "        lastsql, lastreason = sql, (reas or whynot)\n",
    "        p += f\"\\n\\n previous output invalid: {lastreason}. Obey all Rules strictly.\"\n",
    "    return lastsql, f\"failed depsite retries: {lastreason}\"\n",
    "\n",
    "#Specifying the schema\n",
    "schem = (\"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT,  moduleid TEXT);\")\n",
    "#empty list for the resuttls\n",
    "rfew = []\n",
    "\n",
    "module_id = None\n",
    "\n",
    "#Looping through category 6 via few-shot prompting: generating results\n",
    "for i, (q, expected_sql) in enumerate(sql_prompts_category6):\n",
    "    term_text = None\n",
    "    #Specfiying the number of examples the model is show (K), excluding the current item\n",
    "    examples = [p for j,p in enumerate(sql_prompts_category6) if j != i][:3]\n",
    "    promptfs = buildfewpromptcat6(examples, q, schem, module_id, k=3)\n",
    "    gensql, status = generatewithretry6(promptfs, q, module_id, maxtries=2)\n",
    "    rfew.append({\n",
    "        \"Description\": q,\n",
    "        \"Prompt\": promptfs,\n",
    "        \"Generated sql\": gensql,\n",
    "        \"Expected sql\": expected_sql.strip(),\n",
    "        \"Status\": status\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "dt6 = pd.DataFrame(rfew)\n",
    "dt6 = dt6.rename(columns={\n",
    "    \"Expected sql\": \"expected_sql\",\n",
    "    \"Generated sql\": \"generated_sql\"\n",
    "})\n",
    "\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "dt6[\"exact match\"] = dt6.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "dt6[\"Sequence match (ASMS)\"] = dt6.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "dt6[\"EMA\"] = dt6.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {dt6['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {dt6['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {dt6['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores\n",
    "dt6_d=add_datacompy_scores(dt6, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "dt6dcprec =add_datacompy_scores(dt6, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "dt6dcrec= add_datacompy_scores(dt6, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "dt6dccol  =add_datacompy_scores(dt6, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "dt6_exc =tagexpairs(dt6, conn_eval)\n",
    "dt6_exc= add_sqlglot_equiv(dt6_exc, execo=True)\n",
    "mask = dt6_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {dt6_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(dt6_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", dt6_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", dt6dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", dt6dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", dt6dccol[\"datacompy_score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YNmcPq_3Wiu4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Category 6 into results to CSV with correct filename\n",
    "dt6.to_csv(\"category6_results.csv\", index=False)\n",
    "# Downloading the CSv inot local device\n",
    "files.download(\"category6_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bDb-q3B5BCQl",
   "metadata": {
    "id": "bDb-q3B5BCQl"
   },
   "source": [
    "# Section 7.0: Zero-shot Prompting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n8xZwA9dcrY",
   "metadata": {
    "id": "0n8xZwA9dcrY"
   },
   "source": [
    "Section 7.1 Category 1 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juaefPw9XWg4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs1 = []  # Empty list to append the obtained results into\n",
    "\n",
    "# Looping through all of category 1 prompts and generating SQL\n",
    "for q, expected_sql in slq_promts_category1:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs1.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "fd1 = pd.DataFrame(rs1)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "fd1[\"exact match\"] = fd1.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "fd1[\"Sequence match (ASMS)\"] = fd1.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "fd1[\"EMA\"] = fd1.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {fd1['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {fd1['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {fd1['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "fd1_dc = add_datacompy_scores(fd1, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "fd1dcprec  = add_datacompy_scores(fd1, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "fd1dcrec= add_datacompy_scores(fd1, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "fd1dccol= add_datacompy_scores(fd1, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "fd1_exc = tagexpairs(fd1, conn_eval)\n",
    "fd1_exc = add_sqlglot_equiv(fd1_exc, execo=True)\n",
    "mask = fd1_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {fd1_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(fd1_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", fd1_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", fd1dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", fd1dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", fd1dccol[\"datacompy_score\"].mean())\n",
    "\n",
    "#Saving the Category 1 zero shot results to CSV\n",
    "fd1.to_csv(\"fd1_category1_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2uNDEWDKgePl",
   "metadata": {
    "id": "2uNDEWDKgePl"
   },
   "source": [
    "Section 7.2 Category 2 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z-bUJYyigUni",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias so gensql and gen_sql_ do not clash\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs2 = []  # Empty list to append the obtained results into\n",
    "\n",
    "# Looping through all of category 2 prompts and generating SQL\n",
    "for q, expected_sql in sql_promts_category2:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs2.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "f2 = pd.DataFrame(rs2)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "f2[\"exact match\"] = f2.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "f2[\"Sequence match (ASMS)\"] = f2.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "f2[\"EMA\"] = f2.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {f2['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {f2['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {f2['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "f2_dc     = add_datacompy_scores(f2, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "f2dcprec  = add_datacompy_scores(f2, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "f2dcrec   = add_datacompy_scores(f2, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "f2dccol   = add_datacompy_scores(f2, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "f2_exc = tagexpairs(f2, conn_eval)\n",
    "f2_exc = add_sqlglot_equiv(f2_exc, execo=True)\n",
    "mask = f2_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {f2_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(f2_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", f2_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", f2dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", f2dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", f2dccol[\"datacompy_score\"].mean())\n",
    "\n",
    "#Saving Category 2 results in CSV fromat\n",
    "f2.to_csv(\"category2_zeoshotresults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zmbI7F4udhxd",
   "metadata": {
    "id": "zmbI7F4udhxd"
   },
   "source": [
    "Section 7.3: Category 3 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oTogr2-mXWcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias so gensql and gen_sql_ do not clash\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs3 = []  # Empty list to append the obtained results into\n",
    "\n",
    "# Looping through all of category 3 prompts and generating SQL\n",
    "for q, expected_sql in sql_promts_category3:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs3.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "f3 = pd.DataFrame(rs3)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "f3[\"exact match\"] = f3.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "f3[\"Sequence match (ASMS)\"] = f3.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "f3[\"EMA\"] = f3.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {f3['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {f3['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {f3['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "f3_dc     = add_datacompy_scores(f3, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "f3dcprec  = add_datacompy_scores(f3, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "f3dcrec   = add_datacompy_scores(f3, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "f3dccol   = add_datacompy_scores(f3, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "f3_exc = tagexpairs(f3, conn_eval)\n",
    "f3_exc = add_sqlglot_equiv(f3_exc, execo=True)\n",
    "mask = f3_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {f3_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(f3_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", f3_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", f3dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", f3dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", f3dccol[\"datacompy_score\"].mean())\n",
    "\n",
    "# Save Category 3 results to CSV with correct filename\n",
    "# f3 .to_csv(\"zerocategory3_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hE527YC7d25_",
   "metadata": {
    "id": "hE527YC7d25_"
   },
   "source": [
    "Section 7.4: Category 4 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iZ4Ds-KMXWZ1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias so gensql and gen_sql_ do not clash\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs4 = []  # Empty list to append the obtained results into\n",
    "\n",
    "# Looping through all of category 4 prompts and generating SQL\n",
    "for q, expected_sql in sql_prompts_category4:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs4.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "f4 = pd.DataFrame(rs4)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "f4[\"exact match\"] = f4.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "f4[\"Sequence match (ASMS)\"] = f4.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "f4[\"EMA\"] = f4.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {f4['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {f4['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {f4['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "f4_dc     = add_datacompy_scores(f4, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "f4dcprec  = add_datacompy_scores(f4, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "f4dcrec   = add_datacompy_scores(f4, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "f4dccol   = add_datacompy_scores(f4, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "f4_exc = tagexpairs(f4, conn_eval)\n",
    "f4_exc = add_sqlglot_equiv(f4_exc, execo=True)\n",
    "mask = f4_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {f4_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(f4_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", f4_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", f4dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", f4dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", f4dccol[\"datacompy_score\"].mean())\n",
    "\n",
    "#Saving Category 4 results in CSV fromat\n",
    "f4.to_csv(\"category4zeroshotresults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JrMJNcQV4vG_",
   "metadata": {
    "id": "JrMJNcQV4vG_"
   },
   "source": [
    "Section 7.5: Category 5 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O2MyHAr0XWRR",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias so gensql and gen_sql_ do not clash\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs5 = []  # Empty list to append the obtained results into\n",
    "\n",
    "# Looping through all of category 5 prompts and generating SQL\n",
    "for q, expected_sql in sql_prompts_category5:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs5.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "f5 = pd.DataFrame(rs5)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "f5[\"exact match\"] = f5.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "f5[\"Sequence match (ASMS)\"] = f5.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "f5[\"EMA\"] = f5.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {f5['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {f5['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {f5['EMA'].mean():.2%}\")\n",
    "#Computing the Datacompy scores,\n",
    "f5_dc     = add_datacompy_scores(f5, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "f5dcprec  = add_datacompy_scores(f5, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "f5dcrec   = add_datacompy_scores(f5, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "f5dccol   = add_datacompy_scores(f5, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "f5_exc = tagexpairs(f5, conn_eval)\n",
    "f5_exc = add_sqlglot_equiv(f5_exc, execo=True)\n",
    "mask = f5_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {f5_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(f5_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", f5_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", f5dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", f5dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", f5dccol[\"datacompy_score\"].mean())\n",
    "#Saving Category 5 results in CSV format\n",
    "f5.to_csv(\"category5zeroshotresults.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sIylBj-BheJB",
   "metadata": {
    "id": "sIylBj-BheJB"
   },
   "source": [
    "CSection 7.6: Category 6 (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1j8owzhhIIO",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an alias so gensql and gen_sql_ do not clash\n",
    "gensql_fn = gensql if callable(gensql) else gensqlguarded\n",
    "# Defing the base of table creation\n",
    "ts = \"CREATE TABLE TRUD_descriptiontable (conceptid TEXT, term TEXT, typeid TEXT, languagecode TEXT)\"\n",
    "rs6 = []  # Empty list to append the obtained results into\n",
    "# Looping through all of category 5 prompts and generating SQL\n",
    "for q, expected_sql in sql_prompts_category6:\n",
    "    prompt = f\"tables:\\n{ts}\\nquery for: {q}\"\n",
    "    gen_sql_text = gensql_fn(prompt)\n",
    "    rs6.append({\n",
    "        \"description\": q,\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_sql\": gen_sql_text.strip(),\n",
    "        \"expected_sql\": expected_sql.strip()\n",
    "    })\n",
    "\n",
    "#converting the gathered results into a dataframe, in order to compute execuitabllity metrics\n",
    "f6 = pd.DataFrame(rs6)\n",
    "#Computing and pringing and Exact mactch, ASMS and Exectuion accuracy\n",
    "f6[\"exact match\"] = f6.apply(lambda r: r[\"generated_sql\"].lower() == r[\"expected_sql\"].lower(), axis=1)\n",
    "f6[\"Sequence match (ASMS)\"] = f6.apply(lambda r: sequencematch(r[\"generated_sql\"], r[\"expected_sql\"]), axis=1)\n",
    "f6[\"EMA\"] = f6.apply(lambda r: EMA(r[\"expected_sql\"], r[\"generated_sql\"], conn_eval), axis=1)\n",
    "print(f\"EMA: Exact match accuracy: {f6['exact match'].mean():.2%}\")\n",
    "print(f\"Average Sequence Match Score: {f6['Sequence match (ASMS)'].mean():.2%}\")\n",
    "print(f\"Execution Accuracy (EA): {f6['EMA'].mean():.2%}\")\n",
    "\n",
    "#Computing the Datacompy scores,\n",
    "f6_dc   = add_datacompy_scores(f6, conn_eval, mode=\"rows\", metric=\"f1\")\n",
    "f6dcprec = add_datacompy_scores(f6, conn_eval, mode=\"rows\", metric=\"precision\")\n",
    "f6dcrec = add_datacompy_scores(f6, conn_eval, mode=\"rows\", metric=\"recall\")\n",
    "f6dccol   = add_datacompy_scores(f6, conn_eval, mode=\"columns\", metric=\"f1\")\n",
    "#Computing the SQLGlot semantic equivalence\n",
    "f6_exc = tagexpairs(f6, conn_eval)\n",
    "f6_exc = add_sqlglot_equiv(f6_exc, execo=True)\n",
    "mask = f6_exc[\"sqlglot_equiv\"].notna()\n",
    "#Printing the Datacompy, SQLGlot semantic equivalence and total number of queries that were able to run against the database\n",
    "print(f\"SQLGlot semantic equivalence:): {f6_exc.loc[mask, 'sqlglot_equiv'].mean():.2%}\")\n",
    "print(f\"Nr of Queries that were able to execute against the databse: {mask.sum()} / {len(f6_exc)}\")\n",
    "print(\"Datacompy for rows, F1 mean:\", f6_dc[\"datacompy_score\"].mean())\n",
    "print(\"datacompy for rows, precision mean:\", f6dcprec[\"datacompy_score\"].mean())\n",
    "print(\"dataCompy for rows, recall mean:\", f6dcrec[\"datacompy_score\"].mean())\n",
    "print(\"Datacompy for columns, F1) mean:\", f6dccol[\"datacompy_score\"].mean())\n",
    "\n",
    "#Saving Category 6 results in CSV format\n",
    "f6.to_csv(\"category6zeroshotresults.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vhf4xKuDpyq6",
   "metadata": {
    "id": "vhf4xKuDpyq6"
   },
   "source": [
    "#Section 8.0: Generating the Visulisatoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XseOVggpxA7F",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Categories labels as the x-axis\n",
    "cat = [\"Cat 1\", \"Cat 2\", \"Cat 3\", \"Cat 4\", \"Cat 5\", \"Cat 6\"]\n",
    "x = np.arange(len(cat))\n",
    "#Formatting the obtained ASMS percentages for Few-shot and Zero-shot to be visulasied\n",
    "few_shot_asms = [78.61, 86.60, 76.04, 83.70, 59.54, 92.05]\n",
    "zero_shot_asms = [68.42, 84.19, 91.32, 74.57, 79.11, 94.8]\n",
    "#initilasing figure\n",
    "plt.figure(figsize=(10,6))\n",
    "#Plotting the lines with thick linewidth and\n",
    "plt.plot(x, few_shot_asms, marker='o', markersize=10, markeredgewidth=1.5, markeredgecolor='black', linewidth=3, color='#4169E1', label=\"Few-Shot Prompting\")\n",
    "plt.plot(x, zero_shot_asms, marker='s', markersize=10, markeredgewidth=1.5, markeredgecolor='black', linewidth=3, color='#DB7093', label=\"Zero-Shot Prompting\")\n",
    "#Further Formatting the figure\n",
    "plt.xticks(x, cat, fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Average Sequence Match Score (%)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"(NL-SQL) Prompt Categories\", fontsize=14, fontweight='bold')\n",
    "plt.title(\"ASMS Comparison: Few-Shot vs Zero-Shot Prompting Across NL-SQL Categories\", fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, frameon=False)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "#Saving the generated figure into a png for display in section 3.0 in the results in the corresponding text\n",
    "plt.savefig(\"ASMSfewZeroShot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "files.download(\"ASMSfewZeroShot.png\") #downloading the file from googel collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BicUSGC_sJT2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReDefining Categories labels as the x-axis\n",
    "cat=[\"Cat 1\", \"Cat 2\", \"Cat 3\", \"Cat 4\", \"Cat 5\", \"Cat 6\"]\n",
    "x = np.arange(len(cat))\n",
    "bw = 0.35  #defining the width\n",
    "# Converting the number of executions into executions rates\n",
    "few_shot_exec = [6/20*100, 18/20*100, 19/20*100, 2/20*100, 0/21*100, 19/19*100]\n",
    "zero_shot_exec = [6/20*100, 5/20*100, 16/20*100, 0/20*100, 11/21*100, 9/19*100]\n",
    "#intisatialsing the figure\n",
    "plt.figure(figsize=(10,6))\n",
    "#Plotting the grouped bars\n",
    "b1 = plt.bar(x - bw/2, few_shot_exec, width=bw, color='#4169E1', label='Few-Shot Prompting')\n",
    "b2 = plt.bar(x + bw/2, zero_shot_exec, width=bw, color='#DB7093', label='Zero-Shot Prompting')\n",
    "\n",
    "#Adding values labels of the executions accuracies to each bar\n",
    "for bar in b1+b2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.1f}%',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Furhther fromatting\n",
    "plt.xticks(x, categories, fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Execution Success Rate (%)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"(NL-SQL) Prompt Categories\", fontsize=14, fontweight='bold')\n",
    "plt.title(\"Execution Success Rate (%): Few-Shot vs Zero-Shot Prompting\", fontsize=16, fontweight='bold')\n",
    "plt.ylim(0, 110)\n",
    "plt.legend(fontsize=12, frameon=False)\n",
    "plt.grid(axis='y', linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "#Saving the generated figure into a png for display in section 3.0 in the results in the corresponding text\n",
    "plt.savefig(\"EQbr.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "files.download(\"EQbr.png\") #downloading the file from googel collab"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
